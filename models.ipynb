{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: MacOSX\n"
     ]
    }
   ],
   "source": [
    "# file load\n",
    "#%matplotlib inline\n",
    "%matplotlib\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train = pd.read_csv(\"raw/train.csv\") # 136*43 matrix\n",
    "test = pd.read_csv(\"raw/test.csv\") # 99999*42 matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert date to days\n",
    "import time\n",
    "from datetime import datetime as dt\n",
    "# train\n",
    "all_diff = []\n",
    "for item in train[\"Open Date\"]:\n",
    "    diff = dt.now() - dt.strptime(item, \"%m/%d/%Y\")\n",
    "    all_diff.append(int(diff.days/365))\n",
    "\n",
    "train['DaysOfRest'] = pd.Series(all_diff)\n",
    "\n",
    "# test\n",
    "all_diff = []\n",
    "for item in test[\"Open Date\"]:\n",
    "    diff = dt.now() - dt.strptime(item, \"%m/%d/%Y\")\n",
    "    all_diff.append(int(diff.days/365))\n",
    "\n",
    "test['DaysOfRest'] = pd.Series(all_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'City', u'Type', u'P1', u'P2', u'P3', u'P4', u'P5', u'P6', u'P7', u'P8', u'P9', u'P10', u'P11', u'P12', u'P13', u'P14', u'P15', u'P16', u'P17', u'P18', u'P19', u'P20', u'P21', u'P22', u'P23', u'P24', u'P25', u'P26', u'P27', u'P28', u'P29', u'P30', u'P31', u'P32', u'P33', u'P34', u'P35', u'P36', u'P37', u'revenue', u'DaysOfRest'], dtype='object')\n",
      "Index([u'City', u'Type', u'P1', u'P2', u'P3', u'P4', u'P5', u'P6', u'P7', u'P8', u'P9', u'P10', u'P11', u'P12', u'P13', u'P14', u'P15', u'P16', u'P17', u'P18', u'P19', u'P20', u'P21', u'P22', u'P23', u'P24', u'P25', u'P26', u'P27', u'P28', u'P29', u'P30', u'P31', u'P32', u'P33', u'P34', u'P35', u'P36', u'P37', u'DaysOfRest'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# drop id and open date\n",
    "useless_features = ['Id','Open Date','City Group']\n",
    "train = train.drop(useless_features, 1)\n",
    "test = test.drop(useless_features, 1)\n",
    "print train.keys()\n",
    "print test.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'City', u'Type', u'P1', u'P2', u'P3', u'P4', u'P5', u'P6', u'P7', u'P8', u'P9', u'P10', u'P11', u'P12', u'P13', u'P14', u'P15', u'P16', u'P17', u'P18', u'P19', u'P20', u'P21', u'P22', u'P23', u'P24', u'P25', u'P26', u'P27', u'P28', u'P29', u'P30', u'P31', u'P32', u'P33', u'P34', u'P35', u'P36', u'P37', u'revenue', u'DaysOfRest', u'max_params'], dtype='object')\n",
      "5     114\n",
      "25     12\n",
      "15      5\n",
      "10      2\n",
      "20      2\n",
      "18      2\n",
      "dtype: int64\n",
      "(137, 42)\n"
     ]
    }
   ],
   "source": [
    "# add new field = max value\n",
    "train_normal = train.copy()\n",
    "\n",
    "names = [name for name in train.keys() if name[0]=='P']\n",
    "max_params = []\n",
    "j = 0\n",
    "for i in range(0,len(train_normal)):\n",
    "    max_value = 0\n",
    "    for name in names:\n",
    "        if train_normal.iloc[i][name] > max_value:\n",
    "            max_value = train_normal.iloc[i][name]\n",
    "    if max_value > 5:\n",
    "        j +=1\n",
    "    #print max_value\n",
    "    max_params.append(max_value)\n",
    "    \n",
    "train_normal['max_params'] = pd.Series(max_params)\n",
    "print train_normal.keys()\n",
    "print train_normal['max_params'].value_counts()\n",
    "print train_normal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert strange data to good\n",
    "names = [name for name in train_normal.keys() if name[0]=='P']\n",
    "multiplier = {'P1':3,'P2':1.5,'P3':1.5,'P4':1.5,'P5':2,'P6':2,'P7':2,'P8':2,'P9':2,'P10':2,'P11':2,'P12':2,'P13':1.5,'P14':3,\n",
    "              'P15':2,'P16':3,'P17':3,'P18':3,'P19':5,'P20':3,'P21':3,'P22':1,'P23':5,'P24':2,'P25':2,'P26':2.5,'P27':2.5,\n",
    "              'P28':2.5,'P29':2.5,'P30':5,'P31':3,'P32':5,'P33':2,'P34':6,'P35':3,'P36':4,'P37':2}\n",
    "\n",
    "for i in range(0,len(train_normal)):\n",
    "    if train_normal.iloc[i]['max_params'] > 5:\n",
    "        for name in names:\n",
    "            #train[item] = train[item].astype('category')\n",
    "            train_normal[name]=train_normal[name].astype(np.float64)\n",
    "            train_normal.loc[i,name] /= multiplier[name]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "5000\n",
      "10000\n",
      "15000\n",
      "20000\n",
      "25000\n",
      "30000\n",
      "35000\n",
      "40000\n",
      "45000\n",
      "50000\n",
      "55000\n",
      "60000\n",
      "65000\n",
      "70000\n",
      "75000\n",
      "80000\n",
      "85000\n",
      "90000\n",
      "95000\n",
      "End\n"
     ]
    }
   ],
   "source": [
    "# normalize test arguments\n",
    "test_normal = test.copy()\n",
    "for i in range(0,len(test)):\n",
    "    for name in names:\n",
    "        val = test_normal.iloc[i][name]\n",
    "        if val % 1 != 0 or val > 5:\n",
    "            test_normal.loc[i,name] /= multiplier[name]\n",
    "        #if name == 'P7' and val == 2: # probably useless\n",
    "        #    test_normal.loc[i,name] /= 2\n",
    "        #if name == 'P29' and val == 5:\n",
    "        #    test_normal.loc[i,name] /= 2.5\n",
    "    if i % 5000 == 0:\n",
    "        print i\n",
    "print \"End\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         City  P1\n",
      "0    İstanbul   4\n",
      "1      Ankara   4\n",
      "2       Other   2\n",
      "3       Other   2\n",
      "4       Other   3\n",
      "5      Ankara   2\n",
      "6    İstanbul   2\n",
      "7    İstanbul   4\n",
      "8       Other   1\n",
      "9       Other   2\n",
      "10      Other   3\n",
      "11   İstanbul   2\n",
      "12     Ankara   2\n",
      "13   İstanbul   4\n",
      "14      Other   2\n",
      "15   İstanbul   4\n",
      "16   İstanbul   3\n",
      "17   İstanbul   2\n",
      "18      İzmir   4\n",
      "19      Other   2\n",
      "20      Other   3\n",
      "21   İstanbul   5\n",
      "22      Other   3\n",
      "23      Other   2\n",
      "24   İstanbul   5\n",
      "25      Other   4\n",
      "26     Ankara   3\n",
      "27      Other   4\n",
      "28   İstanbul   4\n",
      "29     Ankara   1\n",
      "..        ...  ..\n",
      "107     Other   4\n",
      "108     Other   1\n",
      "109    Ankara   2\n",
      "110     Other   2\n",
      "111     Other   2\n",
      "112  İstanbul   4\n",
      "113     Other   3\n",
      "114     Other   4\n",
      "115  İstanbul   2\n",
      "116     İzmir   2\n",
      "117    Ankara   3\n",
      "118  İstanbul   4\n",
      "119  İstanbul   4\n",
      "120     Other   2\n",
      "121     İzmir   2\n",
      "122  İstanbul   4\n",
      "123     Other   1\n",
      "124    Ankara   1\n",
      "125     Other   3\n",
      "126     Other   3\n",
      "127     Other   3\n",
      "128    Ankara   2\n",
      "129     Other   4\n",
      "130     Other   3\n",
      "131    Ankara   3\n",
      "132     Other   2\n",
      "133     İzmir   4\n",
      "134     Other   3\n",
      "135  İstanbul   4\n",
      "136  İstanbul   4\n",
      "\n",
      "[137 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print train_normal.loc[:,['City','P1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************\n",
      "Nearest item\n",
      "0 23.0 104 1186025.0\n",
      "**************\n",
      "Nearest item\n",
      "1 4.0 91 4839684.0\n",
      "**************\n",
      "Nearest item\n",
      "2 4.0 95 784880.0\n",
      "**************\n",
      "Nearest item\n",
      "3 16.0 110 278575.0\n",
      "**************\n",
      "Nearest item\n",
      "4 23.0 68 479994.0\n",
      "**************\n",
      "Nearest item\n",
      "5 0.0 109 798056.0\n",
      "**************\n",
      "Nearest item\n",
      "6 15.0 115 283650.0\n",
      "**************\n",
      "Nearest item\n",
      "7 7.0 56 2101073.0\n",
      "**************\n",
      "Nearest item\n",
      "8 27.0 115 69512.0\n",
      "**************\n",
      "Nearest item\n",
      "9 8.0 69 1991845.0\n",
      "**************\n",
      "Nearest item\n",
      "10 8.0 129 194669.0\n",
      "**************\n",
      "Nearest item\n",
      "11 17.0 115 278385.0\n",
      "**************\n",
      "Nearest item\n",
      "12 5.0 76 2855789.0\n",
      "**************\n",
      "Nearest item\n",
      "13 3.0 59 1207279.0\n",
      "**************\n",
      "Nearest item\n",
      "14 4.0 76 2591326.0\n",
      "**************\n",
      "Nearest item\n",
      "15 23.0 71 917507.0\n",
      "**************\n",
      "Nearest item\n",
      "16 0.0 38 14235239.0\n",
      "**************\n",
      "Nearest item\n",
      "17 7.0 95 6943025.0\n",
      "**************\n",
      "Nearest item\n",
      "18 18.0 79 975695.0\n",
      "**************\n",
      "Nearest item\n",
      "19 6.0 121 1423142.0\n",
      "**************\n",
      "Nearest item\n",
      "20 27.0 107 2277075.0\n",
      "**************\n",
      "Nearest item\n",
      "21 7.0 56 1240664.0\n",
      "**************\n",
      "Nearest item\n",
      "22 4.0 134 1411229.0\n",
      "**************\n",
      "Nearest item\n",
      "23 6.0 116 5895530.0\n",
      "**************\n",
      "Nearest item\n",
      "24 25.0 107 5655424.0\n",
      "**************\n",
      "Nearest item\n",
      "25 6.0 135 3439013.0\n",
      "**************\n",
      "Nearest item\n",
      "26 20.0 60 897547.0\n",
      "**************\n",
      "Nearest item\n",
      "27 6.0 48 1766508.0\n",
      "**************\n",
      "Nearest item\n",
      "28 25.0 79 1607276.0\n",
      "**************\n",
      "Nearest item\n",
      "29 9.0 106 1440884.0\n",
      "**************\n",
      "Nearest item\n",
      "30 5.0 123 1255430.0\n",
      "**************\n",
      "Nearest item\n",
      "31 24.0 84 2191909.0\n",
      "**************\n",
      "Nearest item\n",
      "32 11.0 132 2525670.0\n",
      "**************\n",
      "Nearest item\n",
      "33 4.0 127 5221070.0\n",
      "**************\n",
      "Nearest item\n",
      "34 8.0 61 249687.0\n",
      "**************\n",
      "Nearest item\n",
      "35 23.0 72 1378509.0\n",
      "**************\n",
      "Nearest item\n",
      "36 7.0 70 954500.0\n",
      "**************\n",
      "Nearest item\n",
      "37 14.0 44 1523489.0\n",
      "**************\n",
      "Nearest item\n",
      "38 0.0 16 14235239.0\n",
      "**************\n",
      "Nearest item\n",
      "39 23.0 74 1682763.0\n",
      "**************\n",
      "Nearest item\n",
      "40 27.0 99 6738741.0\n",
      "**************\n",
      "Nearest item\n",
      "41 25.0 99 9020987.0\n",
      "**************\n",
      "Nearest item\n",
      "42 7.0 129 1012451.0\n",
      "**************\n",
      "Nearest item\n",
      "43 3.0 95 1462146.0\n",
      "**************\n",
      "Nearest item\n",
      "44 14.0 37 1523489.0\n",
      "**************\n",
      "Nearest item\n",
      "45 8.0 121 307809.0\n",
      "**************\n",
      "Nearest item\n",
      "46 22.0 60 1085786.0\n",
      "**************\n",
      "Nearest item\n",
      "47 26.0 114 3709993.0\n",
      "**************\n",
      "Nearest item\n",
      "48 6.0 92 419359.0\n",
      "**************\n",
      "Nearest item\n",
      "49 4.0 59 5030215.0\n",
      "**************\n",
      "Nearest item\n",
      "50 24.0 78 1715246.0\n",
      "**************\n",
      "Nearest item\n",
      "51 6.0 120 794525.0\n",
      "**************\n",
      "Nearest item\n",
      "52 4.0 106 3125543.0\n",
      "**************\n",
      "Nearest item\n",
      "53 6.0 113 3498245.0\n",
      "**************\n",
      "Nearest item\n",
      "54 12.0 23 1652798.0\n",
      "**************\n",
      "Nearest item\n",
      "55 3.0 128 4557333.0\n",
      "**************\n",
      "Nearest item\n",
      "56 7.0 21 1240664.0\n",
      "**************\n",
      "Nearest item\n",
      "57 5.0 55 3682336.0\n",
      "**************\n",
      "Nearest item\n",
      "58 5.0 113 907407.0\n",
      "**************\n",
      "Nearest item\n",
      "59 3.0 13 1207279.0\n",
      "**************\n",
      "Nearest item\n",
      "60 20.0 26 897547.0\n",
      "**************\n",
      "Nearest item\n",
      "61 8.0 96 2062050.0\n",
      "**************\n",
      "Nearest item\n",
      "62 24.0 4 969497.0\n",
      "**************\n",
      "Nearest item\n",
      "63 6.0 111 1373386.0\n",
      "**************\n",
      "Nearest item\n",
      "64 19.0 83 3414941.0\n",
      "**************\n",
      "Nearest item\n",
      "65 5.0 95 485570.0\n",
      "**************\n",
      "Nearest item\n",
      "66 8.0 123 2381498.0\n",
      "**************\n",
      "Nearest item\n",
      "67 9.0 75 12496331.0\n",
      "**************\n",
      "Nearest item\n",
      "68 19.0 115 1046264.0\n",
      "**************\n",
      "Nearest item\n",
      "69 4.0 33 1081180.0\n",
      "**************\n",
      "Nearest item\n",
      "70 5.0 13 824979.0\n",
      "**************\n",
      "Nearest item\n",
      "71 20.0 112 352195.0\n",
      "**************\n",
      "Nearest item\n",
      "72 22.0 114 1630060.0\n",
      "**************\n",
      "Nearest item\n",
      "73 8.0 75 13201297.0\n",
      "**************\n",
      "Nearest item\n",
      "74 23.0 39 1682763.0\n",
      "**************\n",
      "Nearest item\n",
      "75 8.0 73 13201297.0\n",
      "**************\n",
      "Nearest item\n",
      "76 4.0 14 2591326.0\n",
      "**************\n",
      "Nearest item\n",
      "77 4.0 131 1294777.0\n",
      "**************\n",
      "Nearest item\n",
      "78 17.0 117 1852559.0\n",
      "**************\n",
      "Nearest item\n",
      "79 18.0 18 975695.0\n",
      "**************\n",
      "Nearest item\n",
      "80 9.0 70 255403.0\n",
      "**************\n",
      "Nearest item\n",
      "81 6.0 103 362894.0\n",
      "**************\n",
      "Nearest item\n",
      "82 6.0 120 260070.0\n",
      "**************\n",
      "Nearest item\n",
      "83 19.0 64 3414941.0\n",
      "**************\n",
      "Nearest item\n",
      "84 24.0 79 2062668.0\n",
      "**************\n",
      "Nearest item\n",
      "85 1.0 38 2033392.0\n",
      "**************\n",
      "Nearest item\n",
      "86 6.0 12 1616749.0\n",
      "**************\n",
      "Nearest item\n",
      "87 10.0 67 1853863.0\n",
      "**************\n",
      "Nearest item\n",
      "88 4.0 106 2863477.0\n",
      "**************\n",
      "Nearest item\n",
      "89 28.0 8 2588019.0\n",
      "**************\n",
      "Nearest item\n",
      "90 5.0 59 174298.0\n",
      "**************\n",
      "Nearest item\n",
      "91 4.0 1 4839684.0\n",
      "**************\n",
      "Nearest item\n",
      "92 6.0 48 419359.0\n",
      "**************\n",
      "Nearest item\n",
      "93 24.0 39 407177.0\n",
      "**************\n",
      "Nearest item\n",
      "94 22.0 68 83001.0\n",
      "**************\n",
      "Nearest item\n",
      "95 3.0 43 1462146.0\n",
      "**************\n",
      "Nearest item\n",
      "96 6.0 102 3674571.0\n",
      "**************\n",
      "Nearest item\n",
      "97 12.0 92 87628.0\n",
      "**************\n",
      "Nearest item\n",
      "98 9.0 86 1006694.0\n",
      "**************\n",
      "Nearest item\n",
      "99 25.0 41 9020987.0\n",
      "**************\n",
      "Nearest item\n",
      "100 8.0 13 4844604.0\n",
      "**************\n",
      "Nearest item\n",
      "101 25.0 107 1509816.0\n",
      "**************\n",
      "Nearest item\n",
      "102 6.0 96 3674571.0\n",
      "**************\n",
      "Nearest item\n",
      "103 1.0 113 2332490.0\n",
      "**************\n",
      "Nearest item\n",
      "104 23.0 79 1845493.0\n",
      "**************\n",
      "Nearest item\n",
      "105 4.0 88 766734.0\n",
      "**************\n",
      "Nearest item\n",
      "106 4.0 123 625145.0\n",
      "**************\n",
      "Nearest item\n",
      "107 24.0 126 767089.0\n",
      "**************\n",
      "Nearest item\n",
      "108 9.0 105 778361.0\n",
      "**************\n",
      "Nearest item\n",
      "109 0.0 5 798056.0\n",
      "**************\n",
      "Nearest item\n",
      "110 16.0 3 278575.0\n",
      "**************\n",
      "Nearest item\n",
      "111 4.0 95 1722570.0\n",
      "**************\n",
      "Nearest item\n",
      "112 20.0 71 352195.0\n",
      "**************\n",
      "Nearest item\n",
      "113 1.0 103 2332490.0\n",
      "**************\n",
      "Nearest item\n",
      "114 22.0 72 1630060.0\n",
      "**************\n",
      "Nearest item\n",
      "115 15.0 6 283650.0\n",
      "**************\n",
      "Nearest item\n",
      "116 6.0 23 5895530.0\n",
      "**************\n",
      "Nearest item\n",
      "117 17.0 78 1852559.0\n",
      "**************\n",
      "Nearest item\n",
      "118 26.0 104 2619902.0\n",
      "**************\n",
      "Nearest item\n",
      "119 6.0 134 1235162.0\n",
      "**************\n",
      "Nearest item\n",
      "120 6.0 82 260070.0\n",
      "**************\n",
      "Nearest item\n",
      "121 6.0 95 2174577.0\n",
      "**************\n",
      "Nearest item\n",
      "122 26.0 28 419300.0\n",
      "**************\n",
      "Nearest item\n",
      "123 4.0 106 625145.0\n",
      "**************\n",
      "Nearest item\n",
      "124 17.0 2 1754628.0\n",
      "**************\n",
      "Nearest item\n",
      "125 24.0 72 2255232.0\n",
      "**************\n",
      "Nearest item\n",
      "126 24.0 107 767089.0\n",
      "**************\n",
      "Nearest item\n",
      "127 4.0 33 5221070.0\n",
      "**************\n",
      "Nearest item\n",
      "128 3.0 55 4557333.0\n",
      "**************\n",
      "Nearest item\n",
      "129 7.0 42 1012451.0\n",
      "**************\n",
      "Nearest item\n",
      "130 6.0 127 4216127.0\n",
      "**************\n",
      "Nearest item\n",
      "131 4.0 77 1294777.0\n",
      "**************\n",
      "Nearest item\n",
      "132 11.0 32 2525670.0\n",
      "**************\n",
      "Nearest item\n",
      "133 7.0 136 2899513.0\n",
      "**************\n",
      "Nearest item\n",
      "134 4.0 22 1411229.0\n",
      "**************\n",
      "Nearest item\n",
      "135 6.0 95 5947135.0\n",
      "**************\n",
      "Nearest item\n",
      "136 7.0 135 854393.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"names = [name for name in train.keys() if name[0]=='P']\n",
    "for i in range(0,len(train)):\n",
    "    lowest_sum = 1000\n",
    "    lowest_j = 0\n",
    "    for j in range(0, len(train)):\n",
    "        cum_sum = 0\n",
    "        for name in names:\n",
    "            cum_sum += np.abs(train.iloc[i][name]- train.iloc[j][name])\n",
    "        #print outlier_index[i], j,  cum_sum\n",
    "        if j != i:\n",
    "            if cum_sum <=lowest_sum:\n",
    "                lowest_sum = cum_sum\n",
    "                lowest_j = j\n",
    "    print \"**************\"\n",
    "    print \"Nearest item\"\n",
    "    print i, lowest_sum, lowest_j, np.abs(train.iloc[i][\"revenue\"] - train.iloc[lowest_j][\"revenue\"])\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'City', u'City Group', u'Type', u'P1', u'P2', u'P3', u'P4', u'P5', u'P6', u'P7', u'P8', u'P9', u'P10', u'P11', u'P12', u'P13', u'P14', u'P15', u'P16', u'P17', u'P18', u'P19', u'P20', u'P21', u'P22', u'P23', u'P24', u'P25', u'P26', u'P27', u'P28', u'P29', u'P30', u'P31', u'P32', u'P33', u'P34', u'P35', u'P36', u'P37', u'DaysOfRest'], dtype='object')\n",
      "Ready 1/299\n",
      "Ready 2/299\n",
      "Ready 3/299\n",
      "Big value: 25 2567\n",
      "Ready 4/299\n",
      "Ready 5/299\n",
      "Ready 6/299\n",
      "Ready 7/299\n",
      "Ready 8/299\n",
      "Ready 9/299\n",
      "Big value: 25 1552\n",
      "Ready 10/299\n",
      "Ready 11/299\n",
      "Ready 12/299\n",
      "Ready 13/299\n",
      "Ready 14/299\n",
      "Ready 15/299\n",
      "Ready 16/299\n",
      "Big value: 25 537\n",
      "Ready 17/299\n",
      "Ready 18/299\n",
      "Ready 19/299\n",
      "Ready 20/299\n",
      "Ready 21/299\n",
      "Ready 22/299\n",
      "Big value: 25 6176\n",
      "Ready 23/299\n",
      "Ready 24/299\n",
      "Ready 25/299\n",
      "Ready 26/299\n",
      "Ready 27/299\n",
      "Big value: 20 550\n",
      "Ready 28/299\n",
      "Ready 29/299\n",
      "Ready 30/299\n",
      "Big value: 25 2601\n",
      "Ready 31/299\n",
      "Ready 32/299\n",
      "Ready 33/299\n",
      "Big value: 20 558\n",
      "Ready 34/299\n",
      "Ready 35/299\n",
      "Ready 36/299\n",
      "Ready 37/299\n",
      "Big value: 25 3123\n",
      "Ready 38/299\n",
      "Ready 39/299\n",
      "Ready 40/299\n",
      "Ready 41/299\n",
      "Ready 42/299\n",
      "Big value: 25 1082\n",
      "Ready 43/299\n",
      "Ready 44/299\n",
      "Big value: 25 3134\n",
      "Ready 45/299\n",
      "Ready 46/299\n",
      "Ready 47/299\n",
      "Ready 48/299\n",
      "Ready 49/299\n",
      "Ready 50/299\n",
      "Ready 51/299\n",
      "Ready 52/299\n",
      "Ready 53/299\n",
      "Ready 54/299\n",
      "Ready 55/299\n",
      "Ready 56/299\n",
      "Ready 57/299\n",
      "Ready 58/299\n",
      "Ready 59/299\n",
      "Ready 60/299\n",
      "Ready 61/299\n",
      "Ready 62/299\n",
      "Ready 63/299\n",
      "Big value: 20 3087\n",
      "Ready 64/299\n",
      "Ready 65/299\n",
      "Ready 66/299\n",
      "Big value: 30 1122\n",
      "Ready 67/299\n",
      "Ready 68/299\n",
      "Ready 69/299\n",
      "Ready 70/299\n",
      "Big value: 12 616\n",
      "Ready 71/299\n",
      "Big value: 10 615\n",
      "Ready 72/299\n",
      "Ready 73/299\n",
      "Ready 74/299\n",
      "Ready 75/299\n",
      "Big value: 25 1136\n",
      "Ready 76/299\n",
      "Big value: 15 6257\n",
      "Ready 77/299\n",
      "Big value: 15 1138\n",
      "Ready 78/299\n",
      "Ready 79/299\n",
      "Ready 80/299\n",
      "Ready 81/299\n",
      "Ready 82/299\n",
      "Ready 83/299\n",
      "Ready 84/299\n",
      "Ready 85/299\n",
      "Ready 86/299\n",
      "Ready 87/299\n",
      "Ready 88/299\n",
      "Big value: 25 645\n",
      "Ready 89/299\n",
      "Ready 90/299\n",
      "Ready 91/299\n",
      "Ready 92/299\n",
      "Big value: 20 3095\n",
      "Ready 93/299\n",
      "Ready 94/299\n",
      "Ready 95/299\n",
      "Ready 96/299\n",
      "Ready 97/299\n",
      "Ready 98/299\n",
      "Ready 99/299\n",
      "Big value: 10 3097\n",
      "Ready 100/299\n",
      "Ready 101/299\n",
      "Ready 102/299\n",
      "Ready 103/299\n",
      "Big value: 25 1181\n",
      "Ready 104/299\n",
      "Ready 105/299\n",
      "Big value: 25 1184\n",
      "Ready 106/299\n",
      "Ready 107/299\n",
      "Ready 108/299\n",
      "Ready 109/299\n",
      "Ready 110/299\n",
      "Ready 111/299\n",
      "Ready 112/299\n",
      "Ready 113/299\n",
      "Ready 114/299\n",
      "Ready 115/299\n",
      "Big value: 25 1201\n",
      "Ready 116/299\n",
      "Ready 117/299\n",
      "Ready 118/299\n",
      "Ready 119/299\n",
      "Ready 120/299\n",
      "Ready 121/299\n",
      "Ready 122/299\n",
      "Ready 123/299\n",
      "Ready 124/299\n",
      "Ready 125/299\n",
      "Ready 126/299\n",
      "Ready 127/299\n",
      "Big value: 25 1220\n",
      "Ready 128/299\n",
      "Ready 129/299\n",
      "Ready 130/299\n",
      "Ready 131/299\n",
      "Ready 132/299\n",
      "Ready 133/299\n",
      "Ready 134/299\n",
      "Ready 135/299\n",
      "Big value: 20 2769\n",
      "Ready 136/299\n",
      "Ready 137/299\n",
      "Big value: 15 1235\n",
      "Ready 138/299\n",
      "Ready 139/299\n",
      "Ready 140/299\n",
      "Ready 141/299\n",
      "Ready 142/299\n",
      "Ready 143/299\n",
      "Ready 144/299\n",
      "Ready 145/299\n",
      "Ready 146/299\n",
      "Ready 147/299\n",
      "Ready 148/299\n",
      "Ready 149/299\n",
      "Big value: 25 600\n",
      "Ready 150/299\n",
      "Ready 151/299\n",
      "Ready 152/299\n",
      "Ready 153/299\n",
      "Ready 154/299\n",
      "Ready 155/299\n",
      "Ready 156/299\n",
      "Ready 157/299\n",
      "Ready 158/299\n",
      "Ready 159/299\n",
      "Ready 160/299\n",
      "Ready 161/299\n",
      "Ready 162/299\n",
      "Ready 163/299\n",
      "Ready 164/299\n",
      "Ready 165/299\n",
      "Ready 166/299\n",
      "Ready 167/299\n",
      "Ready 168/299\n",
      "Ready 169/299\n",
      "Ready 170/299\n",
      "Big value: 10 1154\n",
      "Ready 171/299\n",
      "Big value: 24 813\n",
      "Ready 172/299\n",
      "Ready 173/299\n",
      "Ready 174/299\n",
      "Ready 175/299\n",
      "Ready 176/299\n",
      "Ready 177/299\n",
      "Ready 178/299\n",
      "Ready 179/299\n",
      "Ready 180/299\n",
      "Ready 181/299\n",
      "Big value: 18 6430\n",
      "Ready 182/299\n",
      "Ready 183/299\n",
      "Ready 184/299\n",
      "Ready 185/299\n",
      "Ready 186/299\n",
      "Ready 187/299\n",
      "Ready 188/299\n",
      "Ready 189/299\n",
      "Ready 190/299\n",
      "Ready 191/299\n",
      "Ready 192/299\n",
      "Ready 193/299\n",
      "Ready 194/299\n",
      "Ready 195/299\n",
      "Ready 196/299\n",
      "Ready 197/299\n",
      "Ready 198/299\n",
      "Ready 199/299\n",
      "Ready 200/299\n",
      "Ready 201/299\n",
      "Ready 202/299\n",
      "Ready 203/299\n",
      "Ready 204/299\n",
      "Ready 205/299\n",
      "Ready 206/299\n",
      "Ready 207/299\n",
      "Big value: 10 2701\n",
      "Ready 208/299\n",
      "Big value: 15 1874\n",
      "Ready 209/299\n",
      "Ready 210/299\n",
      "Ready 211/299\n",
      "Ready 212/299\n",
      "Ready 213/299\n",
      "Big value: 10 2395\n",
      "Ready 214/299\n",
      "Ready 215/299\n",
      "Ready 216/299\n",
      "Big value: 20 2539\n",
      "Ready 217/299\n",
      "Ready 218/299\n",
      "Big value: 25 1765\n",
      "Ready 219/299\n",
      "Ready 220/299\n",
      "Ready 221/299\n",
      "Big value: 20 1169\n",
      "Ready 222/299\n",
      "Ready 223/299\n",
      "Big value: 10 2924\n",
      "Ready 224/299\n",
      "Big value: 25 1170\n",
      "Ready 225/299\n",
      "Ready 226/299\n",
      "Ready 227/299\n",
      "Ready 228/299\n",
      "Ready 229/299\n",
      "Ready 230/299\n",
      "Ready 231/299\n",
      "Ready 232/299\n",
      "Ready 233/299\n",
      "Ready 234/299\n",
      "Ready 235/299\n",
      "Ready 236/299\n",
      "Ready 237/299\n",
      "Ready 238/299\n",
      "Big value: 25 1175\n",
      "Ready 239/299\n",
      "Ready 240/299\n",
      "Ready 241/299\n",
      "Ready 242/299\n",
      "Ready 243/299\n",
      "Ready 244/299\n",
      "Ready 245/299\n",
      "Ready 246/299\n",
      "Ready 247/299\n",
      "Ready 248/299\n",
      "Ready 249/299\n",
      "Ready 250/299\n",
      "Ready 251/299\n",
      "Big value: 20 2476\n",
      "Ready 252/299\n",
      "Ready 253/299\n",
      "Ready 254/299\n",
      "Ready 255/299\n",
      "Ready 256/299\n",
      "Ready 257/299\n",
      "Ready 258/299\n",
      "Big value: 10 1973\n",
      "Ready 259/299\n",
      "Ready 260/299\n",
      "Big value: 15 1467\n",
      "Ready 261/299\n",
      "Big value: 10 6218\n",
      "Ready 262/299\n",
      "Ready 263/299\n",
      "Ready 264/299\n",
      "Ready 265/299\n",
      "Big value: 20 3530\n",
      "Ready 266/299\n",
      "Ready 267/299\n",
      "Ready 268/299\n",
      "Ready 269/299\n",
      "Ready 270/299\n",
      "Ready 271/299\n",
      "Ready 272/299\n",
      "Ready 273/299\n",
      "Ready 274/299\n",
      "Big value: 15 2192\n",
      "Ready 275/299\n",
      "Ready 276/299\n",
      "Ready 277/299\n",
      "Ready 278/299\n",
      "Ready 279/299\n",
      "Ready 280/299\n",
      "Ready 281/299\n",
      "Ready 282/299\n",
      "Ready 283/299\n",
      "Ready 284/299\n",
      "Ready 285/299\n",
      "Ready 286/299\n",
      "Ready 287/299\n",
      "Ready 288/299\n",
      "Ready 289/299\n",
      "Ready 290/299\n",
      "Ready 291/299\n",
      "Ready 292/299\n",
      "Ready 293/299\n",
      "Ready 294/299\n",
      "Big value: 25 2039\n",
      "Ready 295/299\n",
      "Ready 296/299\n",
      "Ready 297/299\n",
      "Ready 298/299\n",
      "Ready 299/299\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'for rest in one_rest[0:5]:\\n    sli =  new_test[new_test[\"DaysOfRest\"]== rest]\\n    for name in names:\\n        print name\\n        val_dict = dict(sli[name].value_counts())\\n        print val_dict'"
      ]
     },
     "execution_count": 518,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# construct new test set\n",
    "\"\"\"new_test = test.copy()\n",
    "names = new_test.keys()[2:]\n",
    "numeric_names = [name for name in names if name[0]=='P']\n",
    "multiplier = {'P1':3,'P2':1.5,'P3':1.5,'P4':1.5,'P5':2,'P6':2,'P7':2,'P8':2,'P9':2,'P10':2,\n",
    "              'P11':2,'P12':2,'P13':1.5,'P14':3,'P15':2,'P16':3,'P17':3,'P18':3,'P19':5,\n",
    "              'P20':3,'P21':3,'P22':1,'P23':5,'P24':2,'P25':2,'P26':2.5,'P27':2.5, 'P28':2.5,\n",
    "              'P29':2.5,'P30':5,'P31':3,'P32':5,'P33':2,'P34':6,'P35':3,'P36':4,'P37':2}        \n",
    "print names\n",
    "i = 1\n",
    "for rest in one_rest:\n",
    "    \n",
    "    sli =  test[test[\"DaysOfRest\"]== rest]\n",
    "    # set to max\n",
    "    for name in names:\n",
    "        val_dict = dict(sli[name].value_counts())\n",
    "        max_key = max(val_dict,key=val_dict.get) \n",
    "        if max_key == 0:\n",
    "            del(val_dict[max_key])\n",
    "            max_key = max(val_dict,key=val_dict.get)\n",
    "        sli.loc[:,name] = max_key\n",
    "    max_value = np.max(sli.iloc[0][numeric_names])\n",
    "    \n",
    "    # convert to normal values\n",
    "    if max_value > 5:\n",
    "        print \"Big value: {value} {revenue}\".format(value = max_value, revenue = rest)\n",
    "        for name in numeric_names:\n",
    "            sli.loc[:,name] = sli.iloc[0][name]/multiplier[name]\n",
    "        \n",
    "    #new_test.update(sli)\n",
    "    print \"Ready {index}/{total}\".format(index = i, total = len(one_rest))\n",
    "    i+=1\n",
    "    \n",
    "for rest in one_rest[0:5]:\n",
    "    sli =  new_test[new_test[\"DaysOfRest\"]== rest]\n",
    "    for name in names:\n",
    "        print name\n",
    "        val_dict = dict(sli[name].value_counts())\n",
    "        print val_dict\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# replace low freq city\n",
    "big_city = ['İstanbul', 'Ankara', 'İzmir']\n",
    "for i in range(0,len(train_normal)):\n",
    "    if train_normal.iloc[i][\"City\"] not in big_city:\n",
    "        train_normal.loc[i,\"City\"] = 'Other'\n",
    "\n",
    "for i in range(0,len(test_normal)):\n",
    "    if test_normal.iloc[i][\"City\"] not in big_city:\n",
    "        test_normal.loc[i,\"City\"] = 'Other'    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130\n"
     ]
    }
   ],
   "source": [
    "# construct more datasets\n",
    "# drop outliers\n",
    "#train_normal = train_normal.drop(\"max_params\",1)\n",
    "outlier_index = [16,23, 74, 98, 99, 115, 132 ]\n",
    "\n",
    "#23, 74, 98, 99, 115, 132    \n",
    "\n",
    "train_clean = train_normal.copy()\n",
    "train_clean = train_clean.drop('max_params',1)\n",
    "#train_clean = train_clean.drop(train_clean.index[outlier_index])\n",
    "#train_clean = train_clean.reset_index(drop=True)\n",
    "print len(train_clean)\n",
    "\n",
    "# all_categorial\n",
    "all_categorial_clean = pd.get_dummies(train_clean)\n",
    "all_categorial_clean[\"DaysOfRest\"] = train_clean[\"DaysOfRest\"]\n",
    "all_categorial_clean = all_categorial_clean.drop('revenue', 1)\n",
    "# all_simple\n",
    "simple_clean = train_clean.copy()\n",
    "simple_clean = pd.get_dummies(train_clean, columns = [\"City\",\"Type\"])\n",
    "simple_clean = simple_clean.drop('revenue', 1)\n",
    "\n",
    "# no date\n",
    "simple_clean_no_date = simple_clean.drop('DaysOfRest', 1)\n",
    "all_categorial_clean_no_date = all_categorial_clean.drop('DaysOfRest', 1)\n",
    "\n",
    "# null_not_null_values\n",
    "\n",
    "null_simple = simple_clean[simple_clean[\"P14\"] == 0]\n",
    "not_null_simple = simple_clean[simple_clean[\"P14\"] != 0]\n",
    "null_simple = null_simple.reset_index(drop=True)\n",
    "not_null_simple = not_null_simple.reset_index(drop=True)\n",
    "\n",
    "null_categorical = train_clean[train_clean[\"P14\"] == 0]\n",
    "not_null_categorical = train_clean[train_clean[\"P14\"] != 0]\n",
    "\n",
    "null_categorical = pd.get_dummies(null_categorical)\n",
    "null_categorical = null_categorical.drop('revenue', 1)\n",
    "not_null_categorical = pd.get_dummies(not_null_categorical)\n",
    "not_null_categorical = not_null_categorical.drop('revenue', 1)\n",
    "\n",
    "\n",
    "null_categorical = null_categorical.reset_index(drop=True)\n",
    "not_null_categorical =not_null_categorical.reset_index(drop=True)\n",
    "\n",
    "null_revenue = train_clean[train_clean[\"P14\"] == 0].revenue \n",
    "null_revenue = null_revenue.reset_index(drop=True)\n",
    "not_null_revenue = train_clean[train_clean[\"P14\"] != 0].revenue\n",
    "not_null_revenue = not_null_revenue.reset_index(drop=True)\n",
    "\n",
    "# normalized_data\n",
    "#================================================================\n",
    "\n",
    "# all_categorial\n",
    "all_categorial_normal = pd.get_dummies(train_normal)\n",
    "all_categorial_normal[\"DaysOfRest\"] = train_normal[\"DaysOfRest\"]\n",
    "all_categorial_normal = all_categorial_normal.drop('revenue', 1)\n",
    "\n",
    "# all_simple\n",
    "simple_normal = train_normal.copy()\n",
    "simple_normal = pd.get_dummies(train_normal, columns = [\"City\",\"Type\"])\n",
    "simple_normal = simple_normal.drop('revenue', 1)\n",
    "\n",
    "# null_not_null\n",
    "train_dummies = pd.get_dummies(train_clean)\n",
    "dummy_revenue = train_dummies[\"revenue\"]\n",
    "null_categorical_normal = train_dummies[train_dummies[\"P14\"] == 0].copy()\n",
    "not_null_categorical_normal = train_dummies[train_dummies[\"P14\"] != 0].copy()\n",
    "\n",
    "null_categorical_normal = null_categorical_normal.drop('revenue', 1)\n",
    "not_null_categorical_normal = not_null_categorical_normal.drop('revenue', 1)\n",
    "train_dummies = train_dummies.drop('revenue', 1)\n",
    "\n",
    "null_categorical_normal = null_categorical_normal.reset_index(drop=True)\n",
    "not_null_categorical_normal = not_null_categorical_normal.reset_index(drop=True)\n",
    "\n",
    "# make all not null forced\n",
    "null_names = [name for name in null_categorical.keys() if null_categorical.iloc[0][name] == 0]\n",
    "forced_null = train_dummies.copy()\n",
    "for name in null_names:\n",
    "    forced_null[name] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'P1', u'P2', u'P3', u'P4', u'P5', u'P6', u'P7', u'P8', u'P9', u'P10', u'P11', u'P12', u'P13', u'P14', u'P15', u'P16', u'P17', u'P18', u'P19', u'P20', u'P21', u'P22', u'P23', u'P24', u'P25', u'P26', u'P27', u'P28', u'P29', u'P30', u'P31', u'P32', u'P33', u'P34', u'P35', u'P36', u'P37', u'DaysOfRest', u'City_Ankara', u'City_Other', u'City_İstanbul', u'City_İzmir', u'Type_DT', u'Type_FC', u'Type_IL'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print train_dummies.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(2)\n",
    "train_third = pd.DataFrame(poly.fit_transform(train_dummies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# imputed values\n",
    "train_imputed = pd.read_csv(\"raw/train_imputed_final.csv\") # 137*43 matrix\n",
    "test_imputed = pd.read_csv(\"raw/test_imputed_final.csv\") # 100000*42 matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(137, 42) (100000, 40)\n",
      "Index([u'City', u'Type', u'P1', u'P2', u'P3', u'P4', u'P5', u'P6', u'P7', u'P8', u'P9', u'P10', u'P11', u'P12', u'P13', u'P14', u'P15', u'P16', u'P17', u'P18', u'P19', u'P20', u'P21', u'P22', u'P23', u'P24', u'P25', u'P26', u'P27', u'P28', u'P29', u'P30', u'P31', u'P32', u'P33', u'P34', u'P35', u'P36', u'P37', u'revenue', u'DaysOfRest', u'max_params'], dtype='object') Index([u'City', u'Type', u'P1', u'P2', u'P3', u'P4', u'P5', u'P6', u'P7', u'P8', u'P9', u'P10', u'P11', u'P12', u'P13', u'P14', u'P15', u'P16', u'P17', u'P18', u'P19', u'P20', u'P21', u'P22', u'P23', u'P24', u'P25', u'P26', u'P27', u'P28', u'P29', u'P30', u'P31', u'P32', u'P33', u'P34', u'P35', u'P36', u'P37', u'DaysOfRest'], dtype='object')\n",
      "Index([u'P1', u'P2', u'P3', u'P4', u'P5', u'P6', u'P7', u'P8', u'P9', u'P10', u'P11', u'P12', u'P13', u'P14', u'P15', u'P16', u'P17', u'P18', u'P19', u'P20', u'P21', u'P22', u'P23', u'P24', u'P25', u'P26', u'P27', u'P28', u'P29', u'P30', u'P31', u'P32', u'P33', u'P34', u'P35', u'P36', u'P37', u'DaysOfRest', u'City_Ankara', u'City_Other', u'City_İstanbul', u'City_İzmir', u'Type_FC', u'Type_IL', u'mean', u'big', u'errors'], dtype='object')\n",
      "Index([u'P1', u'P2', u'P3', u'P4', u'P5', u'P6', u'P7', u'P8', u'P9', u'P10', u'P11', u'P12', u'P13', u'P14', u'P15', u'P16', u'P17', u'P18', u'P19', u'P20', u'P21', u'P22', u'P23', u'P24', u'P25', u'P26', u'P27', u'P28', u'P29', u'P30', u'P31', u'P32', u'P33', u'P34', u'P35', u'P36', u'P37', u'DaysOfRest', u'City_Ankara', u'City_Other', u'City_İstanbul', u'City_İzmir', u'Type_FC', u'Type_IL', u'mean'], dtype='object')\n",
      "set(['big', 'errors'])\n",
      "Index([u'P1', u'P2', u'P3', u'P4', u'P5', u'P6', u'P7', u'P8', u'P9', u'P10', u'P11', u'P12', u'P13', u'P14', u'P15', u'P16', u'P17', u'P18', u'P19', u'P20', u'P21', u'P22', u'P23', u'P24', u'P25', u'P26', u'P27', u'P28', u'P29', u'P30', u'P31', u'P32', u'P33', u'P34', u'P35', u'P36', u'P37', u'DaysOfRest', u'City_Ankara', u'City_Other', u'City_İstanbul', u'City_İzmir', u'Type_FC', u'Type_IL', u'mean', u'big', u'errors'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print train_imputed.shape, test_imputed.shape\n",
    "print train_imputed.keys(), test_imputed.keys()\n",
    "outlier_index = [16]\n",
    "\n",
    "#23, 74, 98, 99, 115, 132    \n",
    "\n",
    "train_clean = train_imputed.copy()\n",
    "train_clean = train_clean.drop(train_clean.index[outlier_index])\n",
    "train_clean = train_clean.reset_index(drop=True)\n",
    "dummy_revenue = train_clean[\"revenue\"]\n",
    "\n",
    "train_clean = train_clean.drop('max_params',1)\n",
    "train_clean = train_clean.drop('revenue',1)\n",
    "\n",
    "#for name in train_clean.keys():\n",
    "#    train_clean[name] = train_clean[name].astype('category')\n",
    "\n",
    "train_clean = pd.get_dummies(train_clean)\n",
    "test_clean = pd.get_dummies(test_imputed)\n",
    "\n",
    "# add mean\n",
    "train_clean[\"mean\"] = pd.Series(np.repeat(np.log(np.mean(dummy_revenue)),len(train_clean)))\n",
    "test_clean[\"mean\"] = pd.Series(np.repeat(np.log(np.mean(dummy_revenue)),len(test_clean)))\n",
    "# add flag for big shit\n",
    "train_clean[\"big\"] = pd.Series(np.repeat(0,len(train_clean)))\n",
    "big_index = dummy_revenue[dummy_revenue>6000000].index\n",
    "train_clean.loc[big_index,\"big\"] = pd.Series(np.repeat(1,len(big_index)), index = big_index)\n",
    "\n",
    "# add flag to error shit\n",
    "# error shit [1, 20, 55, 98, 115, 23, 74, 16, 132, 99, 78, 84, 63, 48, 46, 95, 117, 47, 71, 9, 96, 126]\n",
    "error_list = [1, 20, 55, 98, 115, 23, 74, 16, 132, 99, 78, 84, 63, 48, 46, 95, 117, 47, 71, 9, 96, 126]\n",
    "\n",
    "train_clean[\"errors\"] = pd.Series(np.repeat(0,len(train_clean)))\n",
    "err_index = pd.Index(error_list)\n",
    "train_clean.loc[err_index,\"errors\"] = pd.Series(np.repeat(1,len(err_index)), index = err_index)\n",
    "\n",
    "print train_clean.keys()\n",
    "print test_clean.keys()\n",
    "print set(train_clean.keys()) - set(test_clean.keys())\n",
    "#train_clean = train_clean.drop(['P1',\"P2\",\"P3\",\"P4\",\"P5\",\"P7\",\"P9\",\"P10\",\"P11\",\"P12\",\"P13\",\n",
    "#                                \"P14\",\"P15\",\"P16\",\"P17\",\"P18\",\"P19\",\"P21\",\"P22\",\"P29\",\"P30\",\"P26\",\"P37\"],1)\n",
    "print train_clean.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn.cross_validation.KFold(n=136, n_folds=10, shuffle=True, random_state=None)\n",
      "============\n",
      "GBM\n",
      "============\n",
      "GradientBoostingRegressor(alpha=0.9, init=None, learning_rate=0.001,\n",
      "             loss='ls', max_depth=9, max_features=None,\n",
      "             max_leaf_nodes=None, min_samples_leaf=3, min_samples_split=2,\n",
      "             min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
      "             random_state=None, subsample=1.0, verbose=0, warm_start=False)\n",
      "RMSE: 3002928.46 Max pred: 6618017.92\n",
      "RMSE: 2258256.66 Max pred: 6260137.01\n",
      "RMSE: 1463265.39 Max pred: 6212384.05\n",
      "RMSE: 1236109.39 Max pred: 6759735.82\n",
      "RMSE: 1116863.68 Max pred: 7074716.12\n",
      "RMSE: 1381542.23 Max pred: 5885646.94\n",
      "RMSE: 1159159.53 Max pred: 5397383.62\n",
      "RMSE: 1543502.96 Max pred: 6541428.21\n",
      "RMSE: 1058200.75 Max pred: 4113404.12\n",
      "RMSE: 803776.66 Max pred: 8235772.01\n",
      "Int64Index([74, 98, 99, 132, 48, 115, 23, 16, 131, 94], dtype='int64')\n",
      "Total errors  16\n",
      "Big errors  16\n",
      "10086136.8421 2408184.4955\n",
      "AVG RMSE: 1502360.57 MAX RMSE: 3002928.46 MIN RMSE: 803776.66\n",
      "============\n",
      "Simple_regression\n",
      "============\n",
      "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)\n",
      "RMSE: 2975598.78 Max pred: 10776914.63\n",
      "RMSE: 1493107.48 Max pred: 14371734.10\n",
      "RMSE: 2089095.98 Max pred: 6885960.02\n",
      "RMSE: 1853983.04 Max pred: 9432694.80\n",
      "RMSE: 2985501.11 Max pred: 15894472.00\n",
      "RMSE: 1414279.09 Max pred: 10193573.86\n",
      "RMSE: 1714643.53 Max pred: 10781437.75\n",
      "RMSE: 1592763.90 Max pred: 7174400.88\n",
      "RMSE: 1192594.17 Max pred: 3692853.77\n",
      "RMSE: 932503.91 Max pred: 7191361.80\n",
      "Int64Index([91, 74, 16, 71, 96, 54, 9, 134, 132, 46], dtype='int64')\n",
      "Total errors  48\n",
      "Big errors  32\n",
      "9112046.997 3525428.53492\n",
      "AVG RMSE: 1824407.10 MAX RMSE: 2985501.11 MIN RMSE: 932503.91\n",
      "============\n",
      "ElasticNet\n",
      "============\n",
      "ElasticNetCV(alphas=None, copy_X=True, cv=8, eps=0.0001, fit_intercept=True,\n",
      "       l1_ratio=1, max_iter=1000, n_alphas=100, n_jobs=1, normalize=True,\n",
      "       positive=False, precompute='auto', random_state=None,\n",
      "       selection='cyclic', tol=0.0001, verbose=0)\n",
      "RMSE: 2549422.02 Max pred: 8290830.16\n",
      "RMSE: 1628068.64 Max pred: 9071438.42\n",
      "RMSE: 1421628.25 Max pred: 6641551.13\n",
      "RMSE: 947553.26 Max pred: 8106634.74\n",
      "RMSE: 1200151.83 Max pred: 8739299.78\n",
      "RMSE: 1152519.79 Max pred: 8293556.17\n",
      "RMSE: 1208201.87 Max pred: 6880107.38\n",
      "RMSE: 1321356.30 Max pred: 7206849.93\n",
      "RMSE: 1167098.82 Max pred: 3645158.57\n",
      "RMSE: 555365.79 Max pred: 7097774.30\n",
      "Int64Index([74, 98, 20, 132, 9, 131, 115, 17, 19, 52], dtype='int64')\n",
      "Total errors  62\n",
      "Big errors  14\n",
      "8258233.83805 2279327.48444\n",
      "AVG RMSE: 1315136.66 MAX RMSE: 2549422.02 MIN RMSE: 555365.79\n",
      "============\n",
      "KNN\n",
      "============\n",
      "KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_neighbors=2, p=2, weights='distance')\n",
      "RMSE: 3519778.00 Max pred: 5292484.02\n",
      "RMSE: 3358874.90 Max pred: 5979679.46\n",
      "RMSE: 2360140.06 Max pred: 6836468.13\n",
      "RMSE: 2268312.79 Max pred: 7530609.90\n",
      "RMSE: 2297286.07 Max pred: 7571064.32\n",
      "RMSE: 1712012.68 Max pred: 6086136.53\n",
      "RMSE: 1441633.72 Max pred: 5555126.14\n",
      "RMSE: 2459240.37 Max pred: 6932768.19\n",
      "RMSE: 1213490.88 Max pred: 4628435.36\n",
      "RMSE: 2101750.71 Max pred: 7718737.04\n",
      "Int64Index([74, 98, 115, 48, 47, 132, 23, 117, 22, 46], dtype='int64')\n",
      "Total errors  100\n",
      "Big errors  38\n",
      "11256579.9775 3941001.0709\n",
      "AVG RMSE: 2273252.02 MAX RMSE: 3519778.00 MIN RMSE: 1213490.88\n",
      "============\n",
      "AdaBoost\n",
      "============\n",
      "AdaBoostRegressor(base_estimator=DecisionTreeRegressor(criterion='mse', max_depth=4, max_features=None,\n",
      "           max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, random_state=None,\n",
      "           splitter='best'),\n",
      "         learning_rate=1.0, loss='square', n_estimators=100,\n",
      "         random_state=None)\n",
      "RMSE: 2630178.79 Max pred: 8630682.00\n",
      "RMSE: 2052431.06 Max pred: 7129426.33\n",
      "RMSE: 1326110.45 Max pred: 6840263.84\n",
      "RMSE: 869205.89 Max pred: 6798135.03\n",
      "RMSE: 1089118.45 Max pred: 7495092.00\n",
      "RMSE: 1190425.43 Max pred: 7401440.70\n",
      "RMSE: 1122952.74 Max pred: 7506298.30\n",
      "RMSE: 1309202.21 Max pred: 7330872.99\n",
      "RMSE: 1075384.19 Max pred: 4316715.00\n",
      "RMSE: 2045017.09 Max pred: 13575224.00\n",
      "Int64Index([74, 39, 98, 9, 52, 18, 132, 117, 86, 99], dtype='int64')\n",
      "Total errors  115\n",
      "Big errors  15\n",
      "8003475.0657 2250909.3008\n",
      "AVG RMSE: 1471002.63 MAX RMSE: 2630178.79 MIN RMSE: 869205.89\n",
      "============\n",
      "ExtraTrees\n",
      "============\n",
      "ExtraTreeRegressor(criterion='mse', max_depth=10, max_features='auto',\n",
      "          max_leaf_nodes=None, min_samples_leaf=3, min_samples_split=3,\n",
      "          min_weight_fraction_leaf=0.0, random_state=None,\n",
      "          splitter='random')\n",
      "RMSE: 2348422.68 Max pred: 9508382.07\n",
      "RMSE: 1953284.90 Max pred: 7565770.54\n",
      "RMSE: 1828486.26 Max pred: 7476223.11\n",
      "RMSE: 1284734.67 Max pred: 7217179.71\n",
      "RMSE: 1085426.74 Max pred: 8405303.77\n",
      "RMSE: 1210565.48 Max pred: 8134211.64\n",
      "RMSE: 1021023.75 Max pred: 8072944.79\n",
      "RMSE: 1541082.46 Max pred: 8633338.81\n",
      "RMSE: 1566789.67 Max pred: 4637159.76\n",
      "RMSE: 1666550.39 Max pred: 10007481.91\n",
      "Int64Index([74, 98, 20, 95, 9, 39, 82, 22, 117, 18], dtype='int64')\n",
      "Total errors  136\n",
      "Big errors  21\n",
      "7040681.93277 2636185.99687\n",
      "AVG RMSE: 1550636.70 MAX RMSE: 2348422.68 MIN RMSE: 1021023.75\n",
      "============\n",
      "RandomForest\n",
      "============\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=5,\n",
      "           max_features='auto', max_leaf_nodes=None, min_samples_leaf=3,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "           verbose=0, warm_start=False)\n",
      "RMSE: 2667403.12 Max pred: 8717943.35\n",
      "RMSE: 1915666.60 Max pred: 7929584.23\n",
      "RMSE: 1255471.42 Max pred: 8204415.62\n",
      "RMSE: 997991.97 Max pred: 8412164.70\n",
      "RMSE: 1448651.87 Max pred: 10543121.04\n",
      "RMSE: 1106657.70 Max pred: 7829519.62\n",
      "RMSE: 1118401.66 Max pred: 7462482.62\n",
      "RMSE: 1190941.02 Max pred: 9007943.47\n",
      "RMSE: 1110880.51 Max pred: 3928482.77\n",
      "RMSE: 1176602.00 Max pred: 10228628.89\n",
      "Int64Index([74, 98, 91, 39, 52, 20, 78, 86, 117, 11], dtype='int64')\n",
      "Total errors  151\n",
      "Big errors  15\n",
      "7831120.65322 2200311.64081\n",
      "AVG RMSE: 1398866.79 MAX RMSE: 2667403.12 MIN RMSE: 997991.97\n",
      "============\n",
      "SVR\n",
      "============\n",
      "SVR(C=1, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=0.0,\n",
      "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
      "RMSE: 3644688.86 Max pred: 5924331.80\n",
      "RMSE: 2977217.21 Max pred: 5479944.35\n",
      "RMSE: 1936388.84 Max pred: 5285541.52\n",
      "RMSE: 1702176.88 Max pred: 4711436.40\n",
      "RMSE: 1839735.00 Max pred: 5115073.92\n",
      "RMSE: 1568531.59 Max pred: 5688078.93\n",
      "RMSE: 1126084.59 Max pred: 5559569.28\n",
      "RMSE: 1920503.88 Max pred: 6159837.79\n",
      "RMSE: 1177007.00 Max pred: 4274602.18\n",
      "RMSE: 1340628.93 Max pred: 4831974.43\n",
      "Int64Index([74, 98, 115, 132, 23, 99, 16, 46, 84, 39], dtype='int64')\n",
      "Total errors  176\n",
      "Big errors  25\n",
      "12339785.6713 3058486.87624\n",
      "AVG RMSE: 1923296.28 MAX RMSE: 3644688.86 MIN RMSE: 1126084.59\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'print \"Build ensemble\"\\n\\nens_pred = ensemble.RandomForestRegressor(n_estimators=600, min_samples_leaf = 3, oob_score=True)\\n#ensemble.GradientBoostingRegressor(**params2)\\n\\nerror_indexes = []\\navg_rmse = []\\nres = np.exp(Y)\\nfor train_index, test_index in kf:\\n    dataset_extended = pd.DataFrame()\\n    for multi_regr, regr_name in all_regr_ens:\\n        #print \"============\"\\n        #print regr_name\\n        #print \"============\"\\n        #print multi_regr\\n        multi_regr.fit(dataset.iloc[train_index], Y[train_index])\\n        pred = np.exp(multi_regr.predict(dataset.iloc[test_index])) \\n        rmse = np.sqrt(np.mean((pred - np.exp(Y[test_index])) ** 2))\\n        #print(\"RMSE: %.2f Max pred: %.2f\" % (rmse,np.max(pred)))     \\n        dataset_extended[regr_name] = multi_regr.predict(dataset)\\n    \\n    ens_pred.fit(dataset_extended.iloc[train_index], Y[train_index])\\n    pred = np.exp(ens_pred.predict(dataset_extended.iloc[test_index])) \\n    rmse = np.sqrt(np.mean((pred - np.exp(Y[test_index])) ** 2))\\n    avg_rmse.append(rmse)\\n    print(\"RMSE: %.2f Max pred: %.2f\" % (rmse,np.max(pred)))\\n    res.loc[test_index] = pred - np.exp(Y[test_index])\\n    \\nres = np.abs(res)\\nres.sort(ascending=False)\\nprint res.index[0:10]\\nerror_indexes.extend(list(res[res>2000000].index))\\nprint \"Total errors \",len(error_indexes)\\nprint \"Big errors \", len(res[res>2000000])\\nprint np.max(res[0:10]), np.min(res[0:10])\\nprint(\"AVG RMSE: %.2f MAX RMSE: %.2f MIN RMSE: %.2f\" % (np.mean(avg_rmse), np.max(avg_rmse), np.min(avg_rmse)))\\n#res.hist()'"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all in one\n",
    "from sklearn import cross_validation, linear_model,ensemble\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeRegressor, ExtraTreeRegressor\n",
    "from sklearn.linear_model.stochastic_gradient import SGDRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn import grid_search\n",
    "\n",
    "residuals = pd.Series()\n",
    "\n",
    "dataset = train_clean.copy()\n",
    "Y = np.log(dummy_revenue)\n",
    "\n",
    "kf = cross_validation.KFold(len(dataset), n_folds=10,shuffle=True)\n",
    "print kf\n",
    "\n",
    "\n",
    "params = {'n_estimators': 1000, 'max_depth': 9, 'min_samples_split': 2,\n",
    "          'learning_rate': 0.001, 'loss': 'ls', 'min_samples_leaf':3}\n",
    "gbm = ensemble.GradientBoostingRegressor(**params)\n",
    "params2 = {'n_estimators': 1000, 'max_depth': 9, 'min_samples_split': 2,\n",
    "          'learning_rate': 0.001, 'loss': 'lad', 'min_samples_leaf':2, 'subsample':0.4}\n",
    "\n",
    "regr = linear_model.LinearRegression()\n",
    "elastic = linear_model.ElasticNetCV(l1_ratio=1, eps=0.0001, n_alphas=100, fit_intercept=True, normalize=True, precompute='auto', max_iter=1000, tol=0.0001, cv=8, copy_X=True)\n",
    "gbm2 = ensemble.GradientBoostingRegressor(**params2)\n",
    "ada_tr = ensemble.AdaBoostRegressor(DecisionTreeRegressor(max_depth=4),n_estimators=100, loss='square')\n",
    "knn = KNeighborsRegressor(n_neighbors=2, p=2, weights='distance')\n",
    "knn2 = KNeighborsRegressor(n_neighbors=3,p=2, weights='uniform')\n",
    "extr = ExtraTreeRegressor(criterion='mse', max_depth=10, min_samples_split=3, min_samples_leaf=3, max_features='auto')\n",
    "rfr = ensemble.RandomForestRegressor(n_estimators=10, max_depth=5, min_samples_leaf = 3)#, max_features=None)#)#,  \n",
    "svr = SVR(C=1, epsilon=0.1)\n",
    "\n",
    "grid_params = {\n",
    "    'n_estimators':[10,100,200,300,500,1000],\n",
    "    'min_samples_leaf':[3],\n",
    "    'max_depth':[2,4,6,10]\n",
    "}\n",
    "\n",
    "#'max_features':('sqrt','auto'), \n",
    "\n",
    "grid = grid_search.GridSearchCV(ensemble.RandomForestRegressor(),grid_params,cv=10)\n",
    "\n",
    "all_regr = [\n",
    "    (gbm, 'GBM'),\n",
    "    #(gbm2, 'GBM2'),\n",
    "    (regr, 'Simple_regression'),\n",
    "    (elastic, 'ElasticNet'),\n",
    "    (knn, 'KNN'),\n",
    "    #(knn2,'KNN2'),\n",
    "    (ada_tr, 'AdaBoost'),\n",
    "    #(grid, 'GridSearch'),\n",
    "    (extr, 'ExtraTrees'),\n",
    "    (rfr,'RandomForest'),\n",
    "     (svr, 'SVR')\n",
    "]\n",
    "all_regr_ens = [\n",
    "    #(gbm, 'GBM'),\n",
    "    #(gbm2, 'GBM2'),\n",
    "    (regr, 'Simple_regression'),\n",
    "    (elastic, 'ElasticNet'),\n",
    "    (knn, 'KNN'),\n",
    "    #(knn2,'KNN2'),\n",
    "    #(ada_tr, 'AdaBoost'),\n",
    "    (extr, 'ExtraTrees'),\n",
    "    (rfr,'RandomForest'),\n",
    "    #(svr, 'SVR')\n",
    "]\n",
    "\n",
    "# grid search\n",
    "#grid.fit(dataset, Y)\n",
    "#print grid.best_estimator_\n",
    "#print grid.best_params_\n",
    "\n",
    "\n",
    "error_indexes = []\n",
    "for multi_regr, regr_name in all_regr:\n",
    "    print \"============\"\n",
    "    print regr_name\n",
    "    print \"============\"\n",
    "    print multi_regr\n",
    "    avg_rmse = []\n",
    "    res = np.exp(Y)\n",
    "    for train_index, test_index in kf:\n",
    "        multi_regr.fit(dataset.iloc[train_index], Y[train_index])\n",
    "        pred = np.exp(multi_regr.predict(dataset.iloc[test_index])) \n",
    "        rmse = np.sqrt(np.mean((pred - np.exp(Y[test_index])) ** 2))\n",
    "        avg_rmse.append(rmse)\n",
    "        res.loc[test_index] = pred - np.exp(Y[test_index])\n",
    "        print(\"RMSE: %.2f Max pred: %.2f\" % (rmse,np.max(pred)))\n",
    "    \n",
    "    res = np.abs(res)\n",
    "    res.sort(ascending=False)\n",
    "    print res.index[0:10]\n",
    "    error_indexes.extend(list(res[res>2000000].index))\n",
    "    print \"Total errors \",len(error_indexes)\n",
    "    print \"Big errors \", len(res[res>2000000])\n",
    "    print np.max(res[0:10]), np.min(res[0:10])\n",
    "    #multi_regr.fit(dataset,Y)\n",
    "    \n",
    "    print(\"AVG RMSE: %.2f MAX RMSE: %.2f MIN RMSE: %.2f\" % (np.mean(avg_rmse), np.max(avg_rmse), np.min(avg_rmse)))\n",
    "    \n",
    "\"\"\"print \"Build ensemble\"\n",
    "\n",
    "ens_pred = ensemble.RandomForestRegressor(n_estimators=600, min_samples_leaf = 3, oob_score=True)\n",
    "#ensemble.GradientBoostingRegressor(**params2)\n",
    "\n",
    "error_indexes = []\n",
    "avg_rmse = []\n",
    "res = np.exp(Y)\n",
    "for train_index, test_index in kf:\n",
    "    dataset_extended = pd.DataFrame()\n",
    "    for multi_regr, regr_name in all_regr_ens:\n",
    "        #print \"============\"\n",
    "        #print regr_name\n",
    "        #print \"============\"\n",
    "        #print multi_regr\n",
    "        multi_regr.fit(dataset.iloc[train_index], Y[train_index])\n",
    "        pred = np.exp(multi_regr.predict(dataset.iloc[test_index])) \n",
    "        rmse = np.sqrt(np.mean((pred - np.exp(Y[test_index])) ** 2))\n",
    "        #print(\"RMSE: %.2f Max pred: %.2f\" % (rmse,np.max(pred)))     \n",
    "        dataset_extended[regr_name] = multi_regr.predict(dataset)\n",
    "    \n",
    "    ens_pred.fit(dataset_extended.iloc[train_index], Y[train_index])\n",
    "    pred = np.exp(ens_pred.predict(dataset_extended.iloc[test_index])) \n",
    "    rmse = np.sqrt(np.mean((pred - np.exp(Y[test_index])) ** 2))\n",
    "    avg_rmse.append(rmse)\n",
    "    print(\"RMSE: %.2f Max pred: %.2f\" % (rmse,np.max(pred)))\n",
    "    res.loc[test_index] = pred - np.exp(Y[test_index])\n",
    "    \n",
    "res = np.abs(res)\n",
    "res.sort(ascending=False)\n",
    "print res.index[0:10]\n",
    "error_indexes.extend(list(res[res>2000000].index))\n",
    "print \"Total errors \",len(error_indexes)\n",
    "print \"Big errors \", len(res[res>2000000])\n",
    "print np.max(res[0:10]), np.min(res[0:10])\n",
    "print(\"AVG RMSE: %.2f MAX RMSE: %.2f MIN RMSE: %.2f\" % (np.mean(avg_rmse), np.max(avg_rmse), np.min(avg_rmse)))\n",
    "#res.hist()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'dict'>\n",
      "9\n",
      "[74, 132, 98, 115, 61, 52, 9, 117, 39]\n"
     ]
    }
   ],
   "source": [
    "#print dummy_revenue[dummy_revenue>6000000].count()\n",
    "#nice = dummy_revenue[dummy_revenue>6000000]\n",
    "#dummy_revenue[dummy_revenue>6000000] = pd.Series(np.repeat(0,len(nice)))\n",
    "#print dummy_revenue[dummy_revenue>6000000]\n",
    "#print pd.Series(reduce(lambda x,y: x+y,error_indexes)).value_counts()\n",
    "errors = pd.Series(error_indexes)\n",
    "dict_errors = dict(errors.value_counts())\n",
    "print type(dict_errors)\n",
    "print len([key for key in errors.value_counts().keys() if errors.value_counts()[key] > 4])\n",
    "print [key for key in errors.value_counts().keys() if errors.value_counts()[key] > 4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100136, 45)\n",
      "errors\n",
      "***********************\n",
      "Train:  RF\n",
      "Extended with  RF  shape  (136, 47)\n",
      "Train:  Ada\n",
      "Extended with  Ada  shape  (136, 48)\n",
      "Train:  KNN\n",
      "Extended with  KNN  shape  (136, 49)\n",
      "Train:  KNN2\n",
      "Extended with  KNN2  shape  (136, 50)\n",
      "Train:  SGD\n",
      "Extended with  SGD  shape  (136, 51)\n",
      "Train consolidated model\n",
      "Nothing to impute. Skip this imputation\n",
      "To predict shape:  (100000, 46)  Train shape  (136, 46)\n",
      "Predict:  RF\n",
      "{0: 92666, 1: 7334}\n",
      "Extended with  RF  shape  (100000, 47)\n",
      "Predict:  Ada\n",
      "{0: 83316, 1: 16684}\n",
      "Extended with  Ada  shape  (100000, 48)\n",
      "Predict:  KNN\n",
      "{0: 99439, 1: 561}\n",
      "Extended with  KNN  shape  (100000, 49)\n",
      "Predict:  KNN2\n",
      "{0: 100000}\n",
      "Extended with  KNN2  shape  (100000, 50)\n",
      "Predict:  SGD\n",
      "{0: 86831, 1: 13169}\n",
      "Extended with  SGD  shape  (100000, 51)\n",
      "Predict consolidated\n",
      "{0: 93276, 1: 6724}\n",
      "Check for imputation\n",
      "{0: 93276, 1: 6724}\n",
      "big\n",
      "***********************\n",
      "Train:  RF\n",
      "Extended with  RF  shape  (136, 47)\n",
      "Train:  Ada\n",
      "Extended with  Ada  shape  (136, 48)\n",
      "Train:  KNN\n",
      "Extended with  KNN  shape  (136, 49)\n",
      "Train:  KNN2\n",
      "Extended with  KNN2  shape  (136, 50)\n",
      "Train:  SGD\n",
      "Extended with  SGD  shape  (136, 51)\n",
      "Train consolidated model\n",
      "Nothing to impute. Skip this imputation\n",
      "To predict shape:  (100000, 46)  Train shape  (136, 46)\n",
      "Predict:  RF\n",
      "{0: 94486, 1: 5514}\n",
      "Extended with  RF  shape  (100000, 47)\n",
      "Predict:  Ada\n",
      "{0: 86578, 1: 13422}\n",
      "Extended with  Ada  shape  (100000, 48)\n",
      "Predict:  KNN\n",
      "{0: 98864, 1: 1136}\n",
      "Extended with  KNN  shape  (100000, 49)\n",
      "Predict:  KNN2\n",
      "{0: 100000}\n",
      "Extended with  KNN2  shape  (100000, 50)\n",
      "Predict:  SGD\n",
      "{0: 76651, 1: 23349}\n",
      "Extended with  SGD  shape  (100000, 51)\n",
      "Predict consolidated\n",
      "{0: 96023, 1: 3977}\n",
      "Check for imputation\n",
      "{0: 96023, 1: 3977}\n"
     ]
    }
   ],
   "source": [
    "#  predict errors and outliers\n",
    "# fill type with mean!!!\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import random\n",
    "\n",
    "normal_features = ['errors', 'big']\n",
    "good_features = test_clean.keys()\n",
    "\n",
    "all_samples = train_clean.copy()\n",
    "all_samples = pd.concat([all_samples,test_clean])\n",
    "good_train = all_samples.loc[:,good_features]\n",
    "print good_train.shape\n",
    "\n",
    "train_normal_imputed = train_clean.copy()\n",
    "test_normal_imputed = test_clean.copy()\n",
    "test_normal_imputed[\"errors\"] = pd.Series(np.repeat(np.nan,len(test_normal_imputed)))\n",
    "test_normal_imputed[\"big\"] = pd.Series(np.repeat(np.nan,len(test_normal_imputed)))\n",
    "\n",
    "for name in normal_features:\n",
    "    print name\n",
    "    print \"***********************\"\n",
    "    # make train set\n",
    "    imp_value = all_samples[name]\n",
    "    train_imp = imp_value[pd.notnull(imp_value)]\n",
    "    \n",
    "    # make baselines\n",
    "    vals = train_imp.value_counts()\n",
    "    top_result = pd.Series(np.repeat(vals.keys()[0],len(train_imp)))\n",
    "    random_result = pd.Series([vals.keys()[random.randint(0,len(vals.keys())-1)] for i in range(0,len(train_imp))])\n",
    "    top_result = top_result.reset_index(drop=True)\n",
    "    random_result = random_result.reset_index(drop=True)\n",
    "    \n",
    "    # get train things \n",
    "    X = good_train.iloc[train_imp.index]\n",
    "    Y = train_imp\n",
    "    X = X.reset_index(drop=True)\n",
    "    Y = Y.reset_index(drop=True)\n",
    "    \n",
    "    # add mean expectation\n",
    "    X[str(name+\"_mean\")] = top_result.reset_index(drop=True)\n",
    "    Y = np.array(Y).astype(int) # fix FFSH problems\n",
    "    \n",
    "    # Create classifiers\n",
    "    params = {'n_estimators':100, 'max_features':None, 'max_depth':6}\n",
    "    rf = RandomForestClassifier(**params)\n",
    "    ada = AdaBoostClassifier(n_estimators=100)\n",
    "    knn = KNeighborsClassifier(n_neighbors=10, weights='distance')\n",
    "    knn2 = KNeighborsClassifier(n_neighbors=100, weights='uniform')\n",
    "    sgd = SGDClassifier(loss = 'log', alpha=0.01, n_iter=200)\n",
    "    all_class = [(rf, 'RF'),\n",
    "                (ada, 'Ada'),\n",
    "                (knn, 'KNN'),\n",
    "                (knn2,'KNN2'),\n",
    "                (sgd, 'SGD')]\n",
    "\n",
    "    # initial models \n",
    "    X_extended = X.copy()\n",
    "    \n",
    "    for multi_class, cl_name in all_class:\n",
    "        print \"Train: \", cl_name\n",
    "        multi_class.fit(X, Y)\n",
    "        X_extended[cl_name] = multi_class.predict(X)\n",
    "        print \"Extended with \",cl_name,\" shape \",X_extended.shape\n",
    "    \n",
    "    # consolidated model  \n",
    "    print \"Train consolidated model\"\n",
    "    cons_class =  OneVsRestClassifier(SGDClassifier(loss = 'log', alpha=0.01, n_iter=200))       \n",
    "    cons_class.fit(X_extended, Y)\n",
    "    \n",
    "    # fill imputed values\n",
    "    for df_to_fill in (train_normal_imputed,test_normal_imputed):\n",
    "        accum_df = df_to_fill.copy()\n",
    "        if 'revenue' in accum_df.keys():\n",
    "            accum_df = accum_df.drop(['revenue','max_params'],1)\n",
    "        goal_col = accum_df[name]\n",
    "        goal_indexes = goal_col[pd.isnull(goal_col)].index\n",
    "        \n",
    "        # if nothing to impute\n",
    "        if len(goal_indexes) == 0:\n",
    "            print \"Nothing to impute. Skip this imputation\"\n",
    "            continue\n",
    "        \n",
    "        accum_df = accum_df.loc[goal_indexes,good_features]\n",
    "        accum_df = pd.get_dummies(accum_df)\n",
    "        # add mean\n",
    "        accum_df[str(name+\"_mean\")] = pd.Series(np.repeat(vals.keys()[0],len(accum_df)), index=goal_indexes)\n",
    "        \n",
    "        # fill missed keys\n",
    "        for null_name in set(X.keys())-set(accum_df.keys()):\n",
    "            accum_df[null_name] = pd.Series(np.repeat(0,len(accum_df)), index=goal_indexes)\n",
    "        \n",
    "        print \"To predict shape: \",accum_df.shape, \" Train shape \", X.shape\n",
    "        \n",
    "        base_accum_df = accum_df.copy()\n",
    "        for multi_class, cl_name in all_class:\n",
    "            print \"Predict: \", cl_name\n",
    "            accum_df[cl_name] = multi_class.predict(base_accum_df)\n",
    "            print dict(accum_df[cl_name].value_counts(dropna=False))\n",
    "            print \"Extended with \",cl_name,\" shape \",accum_df.shape\n",
    "        \n",
    "        print \"Predict consolidated\"\n",
    "        \n",
    "        # predict consolidated\n",
    "        df_to_fill.loc[goal_indexes, name] = cons_class.predict(accum_df)\n",
    "        print dict(pd.Series(cons_class.predict(accum_df)).value_counts())\n",
    "        \n",
    "        print \"Check for imputation\"\n",
    "        print dict(df_to_fill[name].value_counts(dropna=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P1\n",
      "{nan: 0, 1.0: 10, 2.0: 42, 3.0: 33, 4.0: 47, 5.0: 4}\n",
      "{1: 5197, 2: 26562, 3: 25335, 4: 35147, 5: 7759}\n",
      "P2\n",
      "{nan: 0, 1.0: 5, 2.0: 13, 3.0: 16, 4.0: 39, 5.0: 63}\n",
      "{nan: 0, 1.0: 1503, 2.0: 8719, 3.0: 17052, 4.0: 23009, 5.0: 49717}\n",
      "P3\n",
      "{nan: 0, 2.0: 3, 3.0: 11, 4.0: 104, 5.0: 18}\n",
      "{nan: 0, 2.0: 1906, 3.0: 9962, 4.0: 78298, 5.0: 9834}\n",
      "P4\n",
      "{nan: 0, 3.0: 22, 4.0: 88, 5.0: 26}\n",
      "{nan: 0, 3.0: 12376, 4.0: 66761, 5.0: 20863}\n",
      "P5\n",
      "{nan: 0, 1.0: 60, 2.0: 64, 3.0: 7, 4.0: 5}\n",
      "{nan: 0, 1.0: 33685, 2.0: 50799, 3.0: 5931, 4.0: 9585}\n",
      "P6\n",
      "{nan: 0, 1.0: 12, 2.0: 61, 3.0: 28, 4.0: 17, 5.0: 18}\n",
      "{1: 9850, 2: 43925, 3: 20554, 4: 16873, 5: 8798}\n",
      "P7\n",
      "{3.5: 4, 1.0: 12, 5.0: 120, nan: 0}\n",
      "{3.5: 3748, 1.0: 9109, 5.0: 87143, nan: 0}\n",
      "P8\n",
      "{nan: 0, 3.0: 7, 4.0: 59, 5.0: 70}\n",
      "{nan: 0, 3.0: 7767, 4.0: 45953, 5.0: 46280}\n",
      "P9\n",
      "{nan: 0, 4.0: 46, 5.0: 90}\n",
      "{nan: 0, 4.0: 41808, 5.0: 58192}\n",
      "P10\n",
      "{nan: 0, 4.0: 40, 5.0: 96}\n",
      "{4: 25209, 5: 74791}\n",
      "P11\n",
      "{nan: 0, 1.0: 22, 2.0: 46, 3.0: 29, 4.0: 18, 5.0: 21}\n",
      "{1: 17781, 2: 23079, 3: 24118, 4: 17188, 5: 17834}\n",
      "P12\n",
      "{nan: 0, 2.0: 4, 3.0: 10, 4.0: 32, 5.0: 90}\n",
      "{2: 1275, 3: 11680, 4: 29330, 5: 57715}\n",
      "P13\n",
      "{nan: 0, 4.0: 38, 5.0: 98}\n",
      "{nan: 0, 4.0: 24198, 5.0: 75802}\n",
      "P14\n",
      "{nan: 0, 1.0: 20, 2.0: 10, 3.0: 89, 4.0: 7, 5.0: 10}\n",
      "{nan: 0, 1.0: 11366, 2.0: 6156, 3.0: 72662, 4.0: 3349, 5.0: 6467}\n",
      "P15\n",
      "{nan: 0, 1.0: 8, 2.0: 41, 3.0: 2, 4.0: 75, 5.0: 10}\n",
      "{nan: 0, 1.0: 1839, 2.0: 30338, 3.0: 3096, 4.0: 57496, 5.0: 7231}\n",
      "P16\n",
      "{nan: 0, 1.0: 1, 2.0: 15, 3.0: 13, 4.0: 85, 5.0: 22}\n",
      "{nan: 0, 1.0: 315, 2.0: 9030, 3.0: 6558, 4.0: 67017, 5.0: 17080}\n",
      "P17\n",
      "{nan: 0, 1.0: 49, 2.0: 12, 3.0: 61, 4.0: 1, 5.0: 13}\n",
      "{nan: 0, 1.0: 28647, 2.0: 8255, 3.0: 48307, 4.0: 3154, 5.0: 11637}\n",
      "P18\n",
      "{nan: 0, 1.0: 3, 3.0: 11, 4.0: 120, 5.0: 2}\n",
      "{nan: 0, 1.0: 2788, 2.0: 322, 3.0: 8124, 4.0: 83800, 5.0: 4966}\n",
      "P19\n",
      "{nan: 0, 1.0: 32, 2.0: 28, 3.0: 30, 4.0: 13, 5.0: 33}\n",
      "{1: 13528, 2: 18473, 3: 22998, 4: 15645, 5: 29356}\n",
      "P20\n",
      "{nan: 0, 1.0: 31, 2.0: 17, 3.0: 18, 4.0: 20, 5.0: 50}\n",
      "{1: 19478, 2: 9712, 3: 12253, 4: 14044, 5: 44513}\n",
      "P21\n",
      "{nan: 0, 1.0: 83, 2.0: 33, 3.0: 7, 4.0: 5, 5.0: 8}\n",
      "{1: 44237, 2: 24419, 3: 19102, 4: 5331, 5: 6911}\n",
      "P22\n",
      "{nan: 0, 1.0: 48, 2.0: 40, 3.0: 26, 4.0: 12, 5.0: 10}\n",
      "{1: 34173, 2: 23561, 3: 18858, 4: 11871, 5: 11537}\n",
      "P23\n",
      "{nan: 0, 1.0: 75, 2.0: 26, 3.0: 12, 4.0: 6, 5.0: 17}\n",
      "{1: 39890, 2: 19779, 3: 13759, 4: 9743, 5: 16829}\n",
      "P24\n",
      "{nan: 0, 1.0: 18, 2.0: 21, 3.0: 46, 4.0: 5, 5.0: 46}\n",
      "{nan: 0, 1.0: 9661, 2.0: 22921, 3.0: 30176, 4.0: 4321, 5.0: 32921}\n",
      "P25\n",
      "{nan: 0, 1.0: 16, 2.0: 29, 3.0: 12, 4.0: 63, 5.0: 16}\n",
      "{nan: 0, 1.0: 10678, 2.0: 16962, 3.0: 8008, 4.0: 44179, 5.0: 20173}\n",
      "P26\n",
      "{nan: 0, 1.0: 25, 2.0: 10, 3.0: 52, 4.0: 12, 5.0: 37}\n",
      "{nan: 0, 1.0: 17788, 2.0: 7487, 3.0: 42785, 4.0: 5517, 5.0: 26423}\n",
      "P27\n",
      "{nan: 0, 1.0: 67, 2.0: 9, 3.0: 17, 4.0: 9, 5.0: 34}\n",
      "{nan: 0, 1.0: 40994, 2.0: 4800, 3.0: 19833, 4.0: 6964, 5.0: 27409}\n",
      "P28\n",
      "{nan: 0, 1.0: 24, 2.0: 53, 3.0: 36, 4.0: 11, 5.0: 12}\n",
      "{nan: 0, 1.0: 17681, 2.0: 29496, 3.0: 23384, 4.0: 11836, 5.0: 17603}\n",
      "P29\n",
      "{nan: 0, 1.0: 17, 2.0: 23, 3.0: 96}\n",
      "{nan: 0, 1.0: 11137, 2.0: 21090, 3.0: 67773}\n",
      "P30\n",
      "{nan: 0, 1.0: 3, 2.0: 1, 3.0: 31, 4.0: 14, 5.0: 87}\n",
      "{nan: 0, 1.0: 2410, 2.0: 2172, 3.0: 27521, 4.0: 6300, 5.0: 61597}\n",
      "P31\n",
      "{nan: 0, 1.0: 37, 2.0: 4, 3.0: 39, 4.0: 3, 5.0: 53}\n",
      "{nan: 0, 1.0: 35305, 2.0: 4257, 3.0: 32233, 4.0: 2242, 5.0: 25963}\n",
      "P32\n",
      "{nan: 0, 1.0: 1, 2.0: 20, 3.0: 41, 4.0: 34, 5.0: 40}\n",
      "{nan: 0, 1.0: 963, 2.0: 13925, 3.0: 31048, 4.0: 26316, 5.0: 27748}\n",
      "P33\n",
      "{nan: 0, 1.0: 1, 2.0: 76, 3.0: 37, 4.0: 20, 5.0: 2}\n",
      "{nan: 0, 1.0: 1523, 2.0: 53824, 3.0: 28817, 4.0: 14809, 5.0: 1027}\n",
      "P34\n",
      "{nan: 0, 2.0: 6, 3.0: 73, 4.0: 52, 5.0: 5}\n",
      "{nan: 0, 1.0: 950, 2.0: 2863, 3.0: 50570, 4.0: 40333, 5.0: 5284}\n",
      "P35\n",
      "{nan: 0, 1.0: 7, 2.0: 1, 4.0: 116, 5.0: 12}\n",
      "{nan: 0, 1.0: 4385, 2.0: 910, 4.0: 82852, 5.0: 11853}\n",
      "P36\n",
      "{nan: 0, 2.0: 1, 3.0: 49, 4.0: 64, 5.0: 22}\n",
      "{nan: 0, 1.0: 313, 2.0: 1564, 3.0: 32727, 4.0: 46773, 5.0: 18623}\n",
      "P37\n",
      "{nan: 0, 1.0: 28, 2.0: 9, 3.0: 74, 4.0: 23, 5.0: 2}\n",
      "{nan: 0, 1.0: 25045, 2.0: 7423, 3.0: 51079, 4.0: 15573, 5.0: 880}\n",
      "DaysOfRest\n",
      "{1: 9, 2: 15, 3: 28, 4: 13, 5: 20, 6: 13, 7: 9, 8: 9, 9: 4, 10: 3, 12: 2, 15: 5, 16: 2, 17: 2, 18: 2}\n",
      "{1: 10362, 2: 13509, 3: 12650, 4: 11490, 5: 10911, 6: 7917, 7: 6767, 8: 6627, 9: 1654, 10: 2220, 11: 1206, 13: 934, 14: 2186, 15: 3363, 16: 1305, 17: 3477, 18: 1538, 19: 1563, 20: 321}\n",
      "City_Ankara\n",
      "{0.0: 117, 1.0: 19, nan: 0}\n",
      "{0.0: 91280, 1.0: 8720, nan: 0}\n",
      "City_Other\n",
      "{0.0: 77, 1.0: 59, nan: 0}\n",
      "{0.0: 49272, 1.0: 50728, nan: 0}\n",
      "City_İstanbul\n",
      "{0.0: 87, 1.0: 49, nan: 0}\n",
      "{0.0: 65913, 1.0: 34087, nan: 0}\n",
      "City_İzmir\n",
      "{0.0: 127, 1.0: 9, nan: 0}\n",
      "{0.0: 93535, 1.0: 6465, nan: 0}\n",
      "Type_FC\n",
      "{0.0: 60, 1.0: 76, nan: 0}\n",
      "{0.0: 41424, 1.0: 58576, nan: 0}\n",
      "Type_IL\n",
      "{0.0: 76, 1.0: 60, nan: 0}\n",
      "{0.0: 58576, 1.0: 41424, nan: 0}\n",
      "mean\n",
      "{nan: 0, 15.283718662043697: 136}\n",
      "{nan: 0, 15.283718662043697: 100000}\n",
      "big\n",
      "{0: 115, 1: 21}\n",
      "{0: 96023, 1: 3977}\n",
      "errors\n",
      "{0: 114, 1: 22}\n",
      "{0: 93276, 1: 6724}\n"
     ]
    }
   ],
   "source": [
    "for name in train_normal_imputed.keys():\n",
    "    print name\n",
    "    print dict(train_normal_imputed[name].value_counts(dropna=False))\n",
    "    print dict(test_normal_imputed[name].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x11b1d3a50>"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = np.log(dummy_revenue)\n",
    "#dataset = train_dummies.drop('revenue', 1)\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "dataset = train_clean\n",
    "#dataset = train_normal_imputed\n",
    "#rfr.fit(dataset,Y)\n",
    "#Y = np.log(train_clean.revenue)\n",
    "ax1.scatter(np.exp(extr.predict(dataset)), np.exp(Y), c='b', marker=\"s\", label='first')\n",
    "ax1.scatter(np.exp(rfr.predict(dataset)),np.exp(Y), c='r', marker=\"o\", label='second')\n",
    "#ax1.scatter(np.exp(elastic.predict(dataset)),np.exp(Y), c='y', marker=\"<\", label='third')\n",
    "ax1.scatter(np.exp(Y),np.exp(Y), c='g', marker=\"*\", label='fourth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make submission\n",
    "dataset = train_normal_imputed\n",
    "extr.fit(dataset,Y)\n",
    "pred = np.exp(extr.predict(test_normal_imputed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     4005606.431186\n",
      "1     3150737.732285\n",
      "2     4005606.431186\n",
      "3     3666884.187724\n",
      "4     3512018.784610\n",
      "5     4403549.807717\n",
      "6     3137970.427244\n",
      "7     4892273.957245\n",
      "8     6681666.468508\n",
      "9     2306968.469774\n",
      "10    2502116.144919\n",
      "11    2298282.065914\n",
      "12    4552207.321924\n",
      "13    2298282.065914\n",
      "14    2306968.469774\n",
      "...\n",
      "99985    2883007.657781\n",
      "99986    4892273.957245\n",
      "99987    8653589.137294\n",
      "99988    3150737.732285\n",
      "99989    3666884.187724\n",
      "99990    4623424.848871\n",
      "99991    3785750.170798\n",
      "99992    2306968.469774\n",
      "99993    3666884.187724\n",
      "99994    3785750.170798\n",
      "99995    6681666.468508\n",
      "99996    2306968.469774\n",
      "99997    4552207.321924\n",
      "99998    4739106.691353\n",
      "99999    3006381.879520\n",
      "Length: 100000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "pred = pd.Series(pred)\n",
    "print pred[pred>600000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P1 3.31509176026\n",
      "P2 1.31708695963\n",
      "P3 0.0\n",
      "P4 0.0\n",
      "P5 2.98660209153\n",
      "P6 8.71813783613\n",
      "P7 0.0\n",
      "P8 7.28500338484\n",
      "P9 0.0\n",
      "P10 1.74993996665\n",
      "P11 3.36932481419\n",
      "P12 0.12612466989\n",
      "P13 3.16352090321\n",
      "P14 1.76744868585\n",
      "P15 3.21152946147\n",
      "P16 3.39281000753\n",
      "P17 7.33103778233\n",
      "P18 0.0\n",
      "P19 0.928106528106\n",
      "P20 5.4452321944\n",
      "P21 0.0\n",
      "P22 5.99512438173\n",
      "P23 12.3107076708\n",
      "P24 7.60128211999\n",
      "P25 42.9265321886\n",
      "P26 7.22246498581\n",
      "P27 15.8863121608\n",
      "P28 33.6444894265\n",
      "P29 0.630166897225\n",
      "P30 0.0\n",
      "P31 10.5830102282\n",
      "P32 6.24371798314\n",
      "P33 7.132139098\n",
      "P34 7.13256807082\n",
      "P35 7.51395299057\n",
      "P36 4.21206011312\n",
      "P37 2.13656653084\n",
      "DaysOfRest 156.696746391\n",
      "City_Ankara 9.95876540147\n",
      "City_Other 0.0\n",
      "City_İstanbul 4.1498440924\n",
      "City_İzmir 0.0\n",
      "Type_FC 0.0\n",
      "Type_IL 0.214575951318\n",
      "mean 0.0\n",
      "big 599.674566705\n",
      "errors 4.02740956609\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(0,len(rfr.feature_importances_)):\n",
    "    \n",
    "    print train_normal_imputed.keys()[i], rfr.feature_importances_[i]*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 2)\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "name = \"./raw/my_submission_7_extr_imputed.csv\"\n",
    "result = open(name, \"w\")\n",
    "writer = csv.writer(result, delimiter=',')\n",
    "writer.writerow([\"Id\",\"Prediction\"])\n",
    "\n",
    "for i in range(0,len(pred)):\n",
    "    writer.writerow([i, pred[i]])\n",
    "result.close()    \n",
    "submis = pd.read_csv(name)\n",
    "print submis.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree:  1243870.48577\n",
      "Regr:  3.78472565211e-08\n",
      "Elastic:  1666974.99961\n",
      "AdaBoost:  754021.842087\n",
      "KNN:  3.15176201539e-09\n",
      "Extra Trees:  896100.081204\n",
      "Random Forest:  1181328.89076\n",
      "SVR:  688749.302114\n",
      "SGD:  inf\n"
     ]
    }
   ],
   "source": [
    "Y = dummy_revenue\n",
    "dataset = train_third\n",
    "\n",
    "pred = np.exp(clf.predict(dataset))\n",
    "rmse = np.sqrt(np.mean((pred - Y) ** 2))\n",
    "print \"Tree: \", rmse\n",
    "\n",
    "pred = np.exp(regr.predict(dataset))\n",
    "rmse = np.sqrt(np.mean((pred - Y) ** 2))\n",
    "print \"Regr: \", rmse\n",
    "\n",
    "pred = np.exp(elastic.predict(dataset))\n",
    "rmse = np.sqrt(np.mean((pred - Y) ** 2))\n",
    "print \"Elastic: \", rmse\n",
    "\n",
    "pred = np.exp(ada_tr.predict(dataset))\n",
    "rmse = np.sqrt(np.mean((pred - Y) ** 2))\n",
    "print \"AdaBoost: \", rmse\n",
    "\n",
    "pred = np.exp(knn.predict(dataset))\n",
    "rmse = np.sqrt(np.mean((pred - Y) ** 2))\n",
    "print \"KNN: \", rmse\n",
    "\n",
    "pred = np.exp(extr.predict(dataset))\n",
    "rmse = np.sqrt(np.mean((pred - Y) ** 2))\n",
    "print \"Extra Trees: \", rmse\n",
    "\n",
    "pred = np.exp(rfr.predict(dataset))\n",
    "rmse = np.sqrt(np.mean((pred - Y) ** 2))\n",
    "print \"Random Forest: \", rmse\n",
    "\n",
    "pred = np.exp(svr.predict(dataset))\n",
    "rmse = np.sqrt(np.mean((pred - Y) ** 2))\n",
    "print \"SVR: \", rmse\n",
    "\n",
    "pred = np.exp(sgd.predict(dataset))\n",
    "rmse = np.sqrt(np.mean((pred - Y) ** 2))\n",
    "print \"SGD: \", rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pred = np.exp(extr.predict(dataset))\n",
    "#for i in range(0, len(pred)):\n",
    "#    print pred[i],Y[i], pred[i]-Y[i]\n",
    "#diff = pred - Y\n",
    "#diff.hist(bins=100)\n",
    "models = [clf, regr, elastic, ada_tr, knn, extr, rfr, svr]\n",
    "fig, axes = plt.subplots(nrows=3, ncols=3)\n",
    "m = axes.flat\n",
    "i = 0\n",
    "for model in models:\n",
    "    pred = np.exp(model.predict(dataset))\n",
    "    diff = pred - Y\n",
    "    m[i].plot(list(pred))#, bins = 100)\n",
    "    i+=1\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4341448.74265\n",
      "2225185.3415\n",
      "     P1  P2  P3  P4  P5  P6  P7  P8  P9  P10   ...     P36  P37  DaysOfRest  \\\n",
      "23    5   5   3   5   2   2   5   5   5    4   ...       5    2           5   \n",
      "74    5   5   4   5   1   4   5   3   4    4   ...       0    0           9   \n",
      "98    4   5   4   4   2   4   5   4   4    4   ...       5    3          15   \n",
      "99    3   5   4   4   2   5   5   5   5    5   ...       0    0           7   \n",
      "115   2   4   4   4   2   2   5   4   4    5   ...       0    0           3   \n",
      "132   4   5   4   4   2   3   5   4   4    5   ...       0    0           8   \n",
      "\n",
      "     City_Ankara  City_Other  City_İstanbul  City_İzmir  Type_DT  Type_FC  \\\n",
      "23             0           0              1           0        0        0   \n",
      "74             0           0              1           0        0        1   \n",
      "98             0           0              1           0        0        0   \n",
      "99             0           0              1           0        0        1   \n",
      "115            0           0              0           1        0        1   \n",
      "132            0           0              0           1        0        1   \n",
      "\n",
      "     Type_IL  \n",
      "23         1  \n",
      "74         0  \n",
      "98         1  \n",
      "99         0  \n",
      "115        0  \n",
      "132        0  \n",
      "\n",
      "[6 rows x 45 columns]\n",
      "P1\n",
      "{2.0: 1, 3.0: 1, 4.0: 2, 5.0: 2}\n",
      "P2\n",
      "{4.0: 1, 5.0: 5}\n",
      "P3\n",
      "{3.0: 1, 4.0: 5}\n",
      "P4\n",
      "{4.0: 4, 5.0: 2}\n",
      "P5\n",
      "{1.0: 1, 2.0: 5}\n",
      "P6\n",
      "{2.0: 2, 3.0: 1, 4.0: 2, 5.0: 1}\n",
      "P7\n",
      "{5.0: 6}\n",
      "P8\n",
      "{3.0: 1, 4.0: 3, 5.0: 2}\n",
      "P9\n",
      "{4.0: 4, 5.0: 2}\n",
      "P10\n",
      "{4.0: 3, 5.0: 3}\n",
      "P11\n",
      "{2.0: 2, 3.0: 1, 5.0: 3}\n",
      "P12\n",
      "{3.0: 2, 4.0: 2, 5.0: 2}\n",
      "P13\n",
      "{4.0: 3, 5.0: 3}\n",
      "P14\n",
      "{0.0: 4, 4.0: 1, 5.0: 1}\n",
      "P15\n",
      "{0.0: 4, 4.0: 1, 5.0: 1}\n",
      "P16\n",
      "{0.0: 4, 4.0: 1, 5.0: 1}\n",
      "P17\n",
      "{0.0: 4, 5.0: 2}\n",
      "P18\n",
      "{0.0: 4, 4.0: 1, 5.0: 1}\n",
      "P19\n",
      "{2.0: 1, 3.0: 1, 4.0: 1, 5.0: 3}\n",
      "P20\n",
      "{2.0: 2, 5.0: 4}\n",
      "P21\n",
      "{1.0: 1, 2.0: 3, 5.0: 2}\n",
      "P22\n",
      "{1.0: 2, 2.0: 2, 3.0: 1, 5.0: 1}\n",
      "P23\n",
      "{1.0: 4, 3.0: 1, 4.0: 1}\n",
      "P24\n",
      "{0.0: 4, 2.0: 1, 5.0: 1}\n",
      "P25\n",
      "{0.0: 4, 2.0: 1, 5.0: 1}\n",
      "P26\n",
      "{0.0: 4, 2.0: 1, 5.0: 1}\n",
      "P27\n",
      "{0.0: 4, 1.0: 1, 5.0: 1}\n",
      "P28\n",
      "{3.0: 3, 4.0: 1, 5.0: 2}\n",
      "P29\n",
      "{0.0: 1, 1.0: 1, 2.0: 1, 3.0: 3}\n",
      "P30\n",
      "{0.0: 4, 5.0: 2}\n",
      "P31\n",
      "{0.0: 4, 5.0: 2}\n",
      "P32\n",
      "{0.0: 4, 4.0: 2}\n",
      "P33\n",
      "{0.0: 4, 2.0: 1, 4.0: 1}\n",
      "P34\n",
      "{0.0: 4, 4.0: 1, 5.0: 1}\n",
      "P35\n",
      "{0.0: 4, 4.0: 1, 5.0: 1}\n",
      "P36\n",
      "{0.0: 4, 5.0: 2}\n",
      "P37\n",
      "{0.0: 4, 2.0: 1, 3.0: 1}\n",
      "DaysOfRest\n",
      "{3: 1, 5: 1, 7: 1, 8: 1, 9: 1, 15: 1}\n",
      "City_Ankara\n",
      "{0.0: 6}\n",
      "City_Other\n",
      "{0.0: 6}\n",
      "City_İstanbul\n",
      "{0.0: 2, 1.0: 4}\n",
      "City_İzmir\n",
      "{0.0: 4, 1.0: 2}\n",
      "Type_DT\n",
      "{0.0: 6}\n",
      "Type_FC\n",
      "{0.0: 2, 1.0: 4}\n",
      "Type_IL\n",
      "{0.0: 4, 1.0: 2}\n"
     ]
    }
   ],
   "source": [
    "print dummy_revenue.mean()\n",
    "print dummy_revenue.std()\n",
    "outliers_index = dummy_revenue[dummy_revenue > dummy_revenue.mean()+2*dummy_revenue.std()].index\n",
    "print train_dummies.iloc[outliers_index]\n",
    "out_slice = train_dummies.iloc[outliers_index]\n",
    "for name in train_dummies.keys():\n",
    "    print name\n",
    "    print dict(out_slice[name].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (137,) (49,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-51746d3137e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/pandas/core/ops.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(left, right, name)\u001b[0m\n\u001b[1;32m    529\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'values'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m                 \u001b[0mlvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m             return left._constructor(wrap_results(na_op(lvalues, rvalues)),\n\u001b[0m\u001b[1;32m    532\u001b[0m                                      \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m                                      dtype=dtype)\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/pandas/core/ops.pyc\u001b[0m in \u001b[0;36mna_op\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    466\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m             result = expressions.evaluate(op, str_rep, x, y,\n\u001b[0;32m--> 468\u001b[0;31m                                           raise_on_error=True, **eval_kwargs)\n\u001b[0m\u001b[1;32m    469\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/pandas/computation/expressions.pyc\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(op, op_str, a, b, raise_on_error, use_numexpr, **eval_kwargs)\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muse_numexpr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         return _evaluate(op, op_str, a, b, raise_on_error=raise_on_error,\n\u001b[0;32m--> 218\u001b[0;31m                          **eval_kwargs)\n\u001b[0m\u001b[1;32m    219\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_evaluate_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_on_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraise_on_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/pandas/computation/expressions.pyc\u001b[0m in \u001b[0;36m_evaluate_standard\u001b[0;34m(op, op_str, a, b, raise_on_error, **eval_kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_TEST_MODE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0m_store_test_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/pandas/core/ops.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m     69\u001b[0m         rmul=arith_method(operator.mul, names('rmul'), op('*'),\n\u001b[1;32m     70\u001b[0m                           default_axis=default_axis, reversed=True),\n\u001b[0;32m---> 71\u001b[0;31m         rsub=arith_method(lambda x, y: y - x, names('rsub'), op('-'),\n\u001b[0m\u001b[1;32m     72\u001b[0m                           default_axis=default_axis, reversed=True),\n\u001b[1;32m     73\u001b[0m         rtruediv=arith_method(lambda x, y: operator.truediv(y, x),\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (137,) (49,) "
     ]
    }
   ],
   "source": [
    "pred = clf.predict(dataset)\n",
    "res = np.abs(np.exp(pred)-np.exp(Y))\n",
    "res = pd.Series(res)\n",
    "res.sort(ascending=False)\n",
    "print res.index[0:20]\n",
    "print res[0:10]\n",
    "print res[len(res)-10:len(res)]\n",
    "print res.mean()\n",
    "print np.log(res.sum())\n",
    "acc = set(res.index[0:20])\n",
    "\n",
    "pred = elastic.predict(dataset)\n",
    "res = np.abs(np.exp(pred)-np.exp(Y))\n",
    "res = pd.Series(res)\n",
    "res.sort(ascending=False)\n",
    "print res.index[0:20]\n",
    "print res[0:10]\n",
    "print res[len(res)-10:len(res)]\n",
    "print res.mean()\n",
    "print np.log(res.sum())\n",
    "acc = acc&set(res.index[0:20])\n",
    "\n",
    "pred = regr.predict(dataset)\n",
    "res = np.abs(np.exp(pred)-np.exp(Y))\n",
    "res = pd.Series(res)\n",
    "res.sort(ascending=False)\n",
    "print res.index[0:20]\n",
    "print res[0:10]\n",
    "print res[len(res)-10:len(res)]\n",
    "print res.mean()\n",
    "print np.log(res.sum())\n",
    "acc = acc&set(res.index[0:20])\n",
    "#print train_clean.iloc[res.idxmax()]\n",
    "#res.hist(bins=50)\n",
    "print acc\n",
    "\n",
    "train_clean.iloc[list(acc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P1\n",
      "Float64Index([4.0, 2.0, 3.0, 1.0, 5.0], dtype='float64')\n",
      "P2\n",
      "Float64Index([5.0, 4.0, 3.0, 2.0, 1.0], dtype='float64')\n",
      "P3\n",
      "Float64Index([4.0, 5.0, 3.0, 2.0, 0.0], dtype='float64')\n",
      "P4\n",
      "Float64Index([4.0, 5.0, 3.0], dtype='float64')\n",
      "P5\n",
      "Float64Index([2.0, 1.0, 3.0, 4.0, 5.0], dtype='float64')\n",
      "P6\n",
      "Float64Index([2.0, 3.0, 5.0, 4.0, 1.0], dtype='float64')\n",
      "P7\n",
      "Float64Index([5.0, 1.0, 4.0, 3.0], dtype='float64')\n",
      "P8\n",
      "Float64Index([5.0, 4.0, 3.0, 2.0, 1.0], dtype='float64')\n",
      "P9\n",
      "Float64Index([5.0, 4.0], dtype='float64')\n",
      "P10\n",
      "Float64Index([5.0, 4.0], dtype='float64')\n",
      "P11\n",
      "Float64Index([2.0, 3.0, 1.0, 5.0, 4.0], dtype='float64')\n",
      "P12\n",
      "Float64Index([5.0, 4.0, 3.0, 2.0], dtype='float64')\n",
      "P13\n",
      "Float64Index([5.0, 4.0, 3.0], dtype='float64')\n",
      "P14\n",
      "Float64Index([0.0, 3.0, 1.0, 2.0, 4.0, 5.0], dtype='float64')\n",
      "P15\n",
      "Float64Index([0.0, 4.0, 2.0, 5.0, 1.0, 3.0], dtype='float64')\n",
      "P16\n",
      "Float64Index([0.0, 4.0, 3.0, 5.0, 2.0, 1.0], dtype='float64')\n",
      "P17\n",
      "Float64Index([0.0, 1.0, 2.0, 5.0, 3.0, 4.0], dtype='float64')\n",
      "P18\n",
      "Float64Index([0.0, 4.0, 3.0, 1.0, 5.0], dtype='float64')\n",
      "P19\n",
      "Float64Index([5.0, 1.0, 3.0, 2.0, 4.0], dtype='float64')\n",
      "P20\n",
      "Float64Index([5.0, 1.0, 4.0, 3.0, 2.0], dtype='float64')\n",
      "P21\n",
      "Float64Index([1.0, 2.0, 5.0, 3.0, 4.0], dtype='float64')\n",
      "P22\n",
      "Float64Index([1.0, 2.0, 3.0, 4.0, 5.0], dtype='float64')\n",
      "P23\n",
      "Float64Index([1.0, 2.0, 5.0, 3.0, 4.0], dtype='float64')\n",
      "P24\n",
      "Float64Index([0.0, 5.0, 3.0, 1.0, 2.0, 4.0], dtype='float64')\n",
      "P25\n",
      "Float64Index([0.0, 2.0, 3.0, 1.0, 4.0, 5.0], dtype='float64')\n",
      "P26\n",
      "Float64Index([0.0, 3.0, 4.0, 1.0, 5.0, 2.0], dtype='float64')\n",
      "P27\n",
      "Float64Index([0.0, 1.0, 5.0, 2.0, 4.0, 3.0], dtype='float64')\n",
      "P28\n",
      "Float64Index([2.0, 3.0, 1.0, 5.0, 4.0], dtype='float64')\n",
      "P29\n",
      "Float64Index([3.0, 2.0, 1.0, 0.0], dtype='float64')\n",
      "P30\n",
      "Float64Index([0.0, 5.0, 4.0, 3.0, 1.0, 2.0], dtype='float64')\n",
      "P31\n",
      "Float64Index([0.0, 5.0, 3.0, 1.0, 2.0, 4.0], dtype='float64')\n",
      "P32\n",
      "Float64Index([0.0, 3.0, 5.0, 4.0, 2.0, 1.0], dtype='float64')\n",
      "P33\n",
      "Float64Index([0.0, 2.0, 3.0, 4.0, 5.0, 1.0], dtype='float64')\n",
      "P34\n",
      "Float64Index([0.0, 3.0, 4.0, 2.0, 5.0], dtype='float64')\n",
      "P35\n",
      "Float64Index([0.0, 4.0, 5.0, 1.0, 2.0], dtype='float64')\n",
      "P36\n",
      "Float64Index([0.0, 3.0, 4.0, 5.0, 2.0], dtype='float64')\n",
      "P37\n",
      "Float64Index([0.0, 3.0, 1.0, 2.0, 4.0, 5.0], dtype='float64')\n"
     ]
    }
   ],
   "source": [
    "for name in names:\n",
    "    print name\n",
    "    print train_normal[name].value_counts().keys()\n",
    "\n",
    "multiplier = {'P1':3,'P2':1.5,'P3':1.5,'P4':1.5,'P5':2,'P6':2,'P7':2,'P8':2,'P9':2,'P10':2,'P11':2,'P12':2,'P13':1.5,'P14':3,\n",
    "              'P15':2,'P16':3,'P17':3,'P18':3,'P19':5,'P20':3,'P21':3,'P22':1,'P23':5,'P24':2,'P25':2,'P26':2.5,'P27':2.5,\n",
    "              'P28':2.5,'P29':2.5,'P30':5,'P31':3,'P32':5,'P33':2,'P34':6,'P35':3,'P36':4,'P37':2}\n",
    "\n",
    "# 3, 4, 7,9,10,12,13,18,29,34,35,36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P4\n",
      "4    89\n",
      "5    26\n",
      "3    22\n",
      "dtype: int64\n",
      "P7\n",
      "5    121\n",
      "1     12\n",
      "4      3\n",
      "3      1\n",
      "dtype: int64\n",
      "P9\n",
      "5    91\n",
      "4    46\n",
      "dtype: int64\n",
      "P18\n",
      "0    88\n",
      "4    35\n",
      "3     9\n",
      "1     3\n",
      "5     2\n",
      "dtype: int64\n",
      "P29\n",
      "3    94\n",
      "2    24\n",
      "1    17\n",
      "0     2\n",
      "dtype: int64\n",
      "P34\n",
      "0    88\n",
      "3    26\n",
      "4    14\n",
      "2     6\n",
      "5     3\n",
      "dtype: int64\n",
      "P36\n",
      "0    88\n",
      "3    28\n",
      "4    12\n",
      "5     8\n",
      "2     1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "missed_values = ['P4','P7','P9','P18','P29', 'P34', 'P36']\n",
    "for name in missed_values:\n",
    "    print name\n",
    "    print train_normal[name].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'P1', u'P2', u'P3', u'P4', u'P5', u'P6', u'P7', u'P8', u'P9', u'P10', u'P11', u'P12', u'P13', u'P14', u'P15', u'P16', u'P17', u'P18', u'P19', u'P20', u'P21', u'P22', u'P23', u'P24', u'P25', u'P26', u'P27', u'P28', u'P29', u'P30', u'P31', u'P32', u'P33', u'P34', u'P35', u'P36', u'P37', u'DaysOfRest', u'max_params', u'City_Adana', u'City_Afyonkarahisar', u'City_Amasya', u'City_Ankara', u'City_Antalya', u'City_Aydın', u'City_Balıkesir', u'City_Bolu', u'City_Bursa', u'City_Denizli', u'City_Diyarbakır', u'City_Edirne', u'City_Elazığ', u'City_Eskişehir', u'City_Gaziantep', u'City_Isparta', u'City_Karabük', u'City_Kastamonu', u'City_Kayseri', u'City_Kocaeli', u'City_Konya', u'City_Kütahya', u'City_Kırklareli', u'City_Muğla', u'City_Osmaniye', u'City_Sakarya', u'City_Samsun', u'City_Tekirdağ', u'City_Tokat', u'City_Trabzon', u'City_Uşak', u'City_İstanbul', u'City_İzmir', u'City_Şanlıurfa', u'City Group_Big Cities', u'City Group_Other', u'Type_DT', u'Type_FC', u'Type_IL'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print all_categorial_normal.keys()\n",
    "all_categorial_normal =all_categorial_normal.drop('max_params',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
