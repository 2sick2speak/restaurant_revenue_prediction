{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(137, 43) (100000, 42)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "# TODO try sneaky \n",
    "train = pd.read_csv(\"raw/train.csv\") # 136*43 matrix\n",
    "test = pd.read_csv(\"raw/test.csv\") #99999*42 matrix\n",
    "print train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(137, 44) (100000, 43)\n"
     ]
    }
   ],
   "source": [
    "# convert date to days\n",
    "import time\n",
    "from datetime import datetime as dt\n",
    "# train\n",
    "all_diff = []\n",
    "for item in train[\"Open Date\"]:\n",
    "    diff = dt.now() - dt.strptime(item, \"%m/%d/%Y\")\n",
    "    all_diff.append(int(diff.days/365))\n",
    "\n",
    "train['DaysOfRest'] = pd.Series(all_diff)\n",
    "\n",
    "# test\n",
    "all_diff = []\n",
    "for item in test[\"Open Date\"]:\n",
    "    diff = dt.now() - dt.strptime(item, \"%m/%d/%Y\")\n",
    "    all_diff.append(int(diff.days/365))\n",
    "\n",
    "test['DaysOfRest'] = pd.Series(all_diff)\n",
    "print train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'City', u'City Group', u'Type', u'P1', u'P2', u'P3', u'P4', u'P5', u'P6', u'P7', u'P8', u'P9', u'P10', u'P11', u'P12', u'P13', u'P14', u'P15', u'P16', u'P17', u'P18', u'P19', u'P20', u'P21', u'P22', u'P23', u'P24', u'P25', u'P26', u'P27', u'P28', u'P29', u'P30', u'P31', u'P32', u'P33', u'P34', u'P35', u'P36', u'P37', u'revenue', u'DaysOfRest'], dtype='object')\n",
      "Index([u'City', u'City Group', u'Type', u'P1', u'P2', u'P3', u'P4', u'P5', u'P6', u'P7', u'P8', u'P9', u'P10', u'P11', u'P12', u'P13', u'P14', u'P15', u'P16', u'P17', u'P18', u'P19', u'P20', u'P21', u'P22', u'P23', u'P24', u'P25', u'P26', u'P27', u'P28', u'P29', u'P30', u'P31', u'P32', u'P33', u'P34', u'P35', u'P36', u'P37', u'DaysOfRest'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# drop insufficient columns\n",
    "train = train.drop('Id', 1)\n",
    "train = train.drop('Open Date', 1)\n",
    "test = test.drop('Id', 1)\n",
    "test = test.drop('Open Date', 1)\n",
    "print train.keys()\n",
    "print test.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'City', u'City Group', u'Type', u'P1', u'P2', u'P3', u'P4', u'P5', u'P6', u'P7', u'P8', u'P9', u'P10', u'P11', u'P12', u'P13', u'P14', u'P15', u'P16', u'P17', u'P18', u'P19', u'P20', u'P21', u'P22', u'P23', u'P24', u'P25', u'P26', u'P27', u'P28', u'P29', u'P30', u'P31', u'P32', u'P33', u'P34', u'P35', u'P36', u'P37', u'revenue', u'DaysOfRest'], dtype='object')\n",
      "(137, 42)\n"
     ]
    }
   ],
   "source": [
    "# normalize arguments\n",
    "train_normal = train.copy()\n",
    "\n",
    "# preprocess train\n",
    "names = [name for name in train.keys() if name[0]=='P']\n",
    "max_params = []\n",
    "j = 0\n",
    "for i in range(0,len(train_normal)):\n",
    "    max_value = 0\n",
    "    for name in names:\n",
    "        if train_normal.iloc[i][name] > max_value:\n",
    "            max_value = train_normal.iloc[i][name]\n",
    "    if max_value > 5:\n",
    "        j +=1\n",
    "    #print max_value\n",
    "    max_params.append(max_value)\n",
    "    \n",
    "train_normal['max_params'] = pd.Series(max_params)\n",
    "\n",
    "print train.keys()\n",
    "print train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize train arguments\n",
    "multiplier = {'P1':3,'P2':1.5,'P3':1.5,'P4':1.5,'P5':2,'P6':2,'P7':2,'P8':2,'P9':2,'P10':2,'P11':2,'P12':2,'P13':1.5,'P14':3,\n",
    "              'P15':2,'P16':3,'P17':3,'P18':3,'P19':5,'P20':3,'P21':3,'P22':1,'P23':5,'P24':2,'P25':2,'P26':2.5,'P27':2.5,\n",
    "              'P28':2.5,'P29':2.5,'P30':5,'P31':3,'P32':5,'P33':2,'P34':6,'P35':3,'P36':4,'P37':2}\n",
    "\n",
    "for i in range(0,len(train_normal)):\n",
    "    if train_normal.iloc[i]['max_params'] > 5:\n",
    "        for name in names:\n",
    "            train_normal[name]=train_normal[name].astype(np.float64)\n",
    "            train_normal.loc[i,name] /= multiplier[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "5000\n",
      "10000\n",
      "15000\n",
      "20000\n",
      "25000\n",
      "30000\n",
      "35000\n",
      "40000\n",
      "45000\n",
      "50000\n",
      "55000\n",
      "60000\n",
      "65000\n",
      "70000\n",
      "75000\n",
      "80000\n",
      "85000\n",
      "90000\n",
      "95000\n",
      "End\n"
     ]
    }
   ],
   "source": [
    "# normalize test arguments\n",
    "test_normal = test.copy()\n",
    "for i in range(0,len(test)):\n",
    "    for name in names:\n",
    "        val = test_normal.iloc[i][name]\n",
    "        if val % 1 != 0 or val > 5:\n",
    "            test_normal.loc[i,name] /= multiplier[name]\n",
    "        #if name == 'P7' and val == 2: # probably useless\n",
    "        #    test_normal.loc[i,name] /= 2\n",
    "        #if name == 'P29' and val == 5:\n",
    "        #    test_normal.loc[i,name] /= 2.5\n",
    "    if i % 5000 == 0:\n",
    "        print i\n",
    "print \"End\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# replace low freq city\n",
    "big_city = ['İstanbul', 'Ankara', 'İzmir']\n",
    "for i in range(0,len(train_normal)):\n",
    "    if train_normal.iloc[i][\"City\"] not in big_city:\n",
    "        train_normal.loc[i,\"City\"] = 'Other'\n",
    "\n",
    "for i in range(0,len(test_normal)):\n",
    "    if test_normal.iloc[i][\"City\"] not in big_city:\n",
    "        test_normal.loc[i,\"City\"] = 'Other'    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop city type\n",
    "meaningless_features = ['City Group','P3','P7','P9','P25','P27','P30','P31','P35','P37']\n",
    "train_normal = train_normal.drop(meaningless_features,1)\n",
    "test_normal = test_normal.drop(meaningless_features,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136\n"
     ]
    }
   ],
   "source": [
    "# remove outlier from train set and max_params\n",
    "outlier_index = [16]\n",
    "train_normal = train_normal.drop('max_params',1)\n",
    "train_normal = train_normal.drop(train_normal.index[outlier_index])\n",
    "train_normal = train_normal.reset_index(drop=True)\n",
    "print len(train_normal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['P14', 'P15', 'P16', 'P17', 'P18', 'P24', 'P25', 'P26', 'P27', 'P30', 'P31', 'P32', 'P33', 'P34', 'P35', 'P36', 'P37']\n",
      "Index([u'City', u'City Group', u'Type', u'P1', u'P2', u'P3', u'P4', u'P5', u'P6', u'P7', u'P8', u'P9', u'P10', u'P11', u'P12', u'P13', u'P14', u'P15', u'P16', u'P17', u'P18', u'P19', u'P20', u'P21', u'P22', u'P23', u'P24', u'P25', u'P26', u'P27', u'P28', u'P29', u'P30', u'P31', u'P32', u'P33', u'P34', u'P35', u'P36', u'P37', u'revenue', u'DaysOfRest', u'max_params'], dtype='object')\n",
      "(136, 43)\n"
     ]
    }
   ],
   "source": [
    "# conversion to categorical variables\n",
    "train_categorial = train_normal.copy()\n",
    "test_categorial = test_normal.copy()\n",
    "names = [name for name in test_categorial.keys() if name[0]=='P']\n",
    "for item in names:\n",
    "    train_categorial[item] = train_categorial[item].astype(np.int64)\n",
    "    #train_categorial[item] = train_categorial[item].astype('category')\n",
    "    test_categorial[item] = test_categorial[item].astype(np.int64)\n",
    "    #test_categorial[item] = test_categorial[item].astype('category')\n",
    "\n",
    "# select all null\n",
    "null_categorical = train_categorial[train_categorial[\"P14\"] == 0]\n",
    "null_categorical = pd.get_dummies(null_categorical)\n",
    "null_categorical = null_categorical.drop('revenue', 1)\n",
    "null_categorical = null_categorical.reset_index(drop=True)\n",
    "null_revenue = train_categorial[train_categorial[\"P14\"] == 0].revenue \n",
    "null_revenue = null_revenue.reset_index(drop=True)\n",
    "\n",
    "# make all not null forced\n",
    "null_names = [name for name in null_categorical.keys() if null_categorical.iloc[0][name] == 0 and name[0]=='P']\n",
    "forced_null = train_categorial.copy()\n",
    "print null_names\n",
    "for name in null_names:\n",
    "    forced_null[name] = 0\n",
    "forced_null = forced_null.drop('revenue', 1)\n",
    "forced_null = pd.get_dummies(forced_null)\n",
    "print train_categorial.keys()\n",
    "print train_categorial.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'P1', u'P2', u'P3', u'P4', u'P5', u'P6', u'P7', u'P8', u'P9', u'P10', u'P11', u'P12', u'P13', u'P14', u'P15', u'P16', u'P17', u'P18', u'P19', u'P20', u'P21', u'P22', u'P23', u'P24', u'P25', u'P26', u'P27', u'P28', u'P29', u'P30', u'P31', u'P32', u'P33', u'P34', u'P35', u'P36', u'P37', u'DaysOfRest', u'City_Ankara', u'City_Other', u'City_İstanbul', u'City_İzmir', u'City Group_Big Cities', u'City Group_Other', u'Type_DT', u'Type_FC', u'Type_IL'], dtype='object')\n",
      "(136, 47)\n"
     ]
    }
   ],
   "source": [
    "# one-hot encoding\n",
    "all_categorial_train = pd.get_dummies(train_categorial)\n",
    "all_categorial_train[\"DaysOfRest\"] = train_categorial[\"DaysOfRest\"]\n",
    "all_categorial_train = all_categorial_train.drop('revenue', 1)\n",
    "all_categorial_train = all_categorial_train.drop('max_params', 1)\n",
    "\n",
    "all_cat_test = pd.get_dummies(test_categorial)\n",
    "all_cat_test[\"DaysOfRest\"] = test_categorial[\"DaysOfRest\"]\n",
    "print all_categorial_train.keys()\n",
    "print all_categorial_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['Type_MB'])\n",
      "set([])\n",
      "set([])\n"
     ]
    }
   ],
   "source": [
    "# print diff in keys and delete them\n",
    "key_diff = set(all_cat_test.keys()) - set(all_categorial_train.keys())\n",
    "print key_diff\n",
    "for name in key_diff:\n",
    "    all_cat_test = all_cat_test.drop(name, 1)\n",
    "print set(all_cat_test.keys()) - set(all_categorial_train.keys())\n",
    "print set(all_categorial_train.keys()) - set(all_cat_test.keys()) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(136, 1176)\n",
      "(100000, 1176)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(2)\n",
    "train_two = pd.DataFrame(poly.fit_transform(all_categorial_train))\n",
    "test_two = pd.DataFrame(poly.fit_transform(all_cat_test))\n",
    "print train_two.shape\n",
    "print test_two.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn.cross_validation.KFold(n=136, n_folds=10, shuffle=True, random_state=None)\n",
      "============\n",
      "Simple regression\n",
      "============\n",
      "RMSE: 3444332.56\n",
      "RMSE: 3582273.91\n",
      "RMSE: 2645345.81\n",
      "RMSE: 70454370.60\n",
      "RMSE: 3255044.55\n",
      "RMSE: 3790895.90\n",
      "RMSE: 2295451.69\n",
      "RMSE: 32751588.37\n",
      "RMSE: 3101765.85\n",
      "RMSE: 4068278.59\n",
      "AVG RMSE: 12938934.78 MAX RMSE: 70454370.60 MIN RMSE: 2295451.69\n",
      "============\n",
      "Elastic net\n",
      "============\n",
      "RMSE: 1408713.52\n",
      "RMSE: 2037927.76\n",
      "RMSE: 1554568.21\n",
      "RMSE: 1527577.15\n",
      "RMSE: 1730022.45\n",
      "RMSE: 2309793.64\n",
      "RMSE: 1252813.05\n",
      "RMSE: 3808869.24\n",
      "RMSE: 2480923.08\n",
      "RMSE: 1853317.42\n",
      "AVG RMSE: 1996452.55 MAX RMSE: 3808869.24 MIN RMSE: 1252813.05\n",
      "============\n",
      "GBM\n",
      "============\n",
      "RMSE: 1328220.62\n",
      "RMSE: 2031495.81\n",
      "RMSE: 1620515.78\n",
      "RMSE: 1486181.59\n",
      "RMSE: 1648296.76\n",
      "RMSE: 2203065.39\n",
      "RMSE: 1372275.08\n",
      "RMSE: 3761522.22\n",
      "RMSE: 2631627.12\n",
      "RMSE: 2019018.97\n",
      "AVG RMSE: 2010221.94 MAX RMSE: 3761522.22 MIN RMSE: 1328220.62\n",
      "============\n",
      "AdaBoost\n",
      "============\n",
      "RMSE: 1375351.76\n",
      "RMSE: 2291729.78\n",
      "RMSE: 1460746.58\n",
      "RMSE: 1222059.19\n",
      "RMSE: 1892626.44\n",
      "RMSE: 3309125.54\n",
      "RMSE: 1583621.40\n",
      "RMSE: 4020152.49\n",
      "RMSE: 2791333.31\n",
      "RMSE: 1566135.39\n",
      "AVG RMSE: 2151288.19 MAX RMSE: 4020152.49 MIN RMSE: 1222059.19\n",
      "============\n",
      "Extra Trees\n",
      "============\n",
      "RMSE: 1960805.69\n",
      "RMSE: 2488230.28\n",
      "RMSE: 2351567.83\n",
      "RMSE: 2364023.58\n",
      "RMSE: 2713653.76\n",
      "RMSE: 3378479.72\n",
      "RMSE: 1014328.75\n",
      "RMSE: 3969918.02\n",
      "RMSE: 3197546.10\n",
      "RMSE: 2571735.81\n",
      "AVG RMSE: 2601028.95 MAX RMSE: 3969918.02 MIN RMSE: 1014328.75\n",
      "============\n",
      "SGD\n",
      "============\n",
      "RMSE: inf\n",
      "RMSE: inf\n",
      "RMSE: inf\n",
      "RMSE: inf\n",
      "RMSE: inf\n",
      "RMSE: inf\n",
      "RMSE: inf\n",
      "RMSE: inf\n",
      "RMSE: inf\n",
      "RMSE: inf\n",
      "AVG RMSE: inf MAX RMSE: inf MIN RMSE: inf\n",
      "============\n",
      "Random Forest\n",
      "============\n",
      "RMSE: 1472699.67\n",
      "RMSE: 2007783.27\n",
      "RMSE: 1593320.41\n",
      "RMSE: 1275549.40\n",
      "RMSE: 1534639.12\n",
      "RMSE: 2141927.57\n",
      "RMSE: 1076484.64\n",
      "RMSE: 3604657.27\n",
      "RMSE: 2751875.51\n",
      "RMSE: 1685927.90\n",
      "AVG RMSE: 1914486.48 MAX RMSE: 3604657.27 MIN RMSE: 1076484.64\n",
      "============\n",
      "SVR\n",
      "============\n",
      "RMSE: 1598104.07\n",
      "RMSE: 2348369.81\n",
      "RMSE: 1443475.66\n",
      "RMSE: 1304832.18\n",
      "RMSE: 1647238.84\n",
      "RMSE: 2517327.33\n",
      "RMSE: 1124963.23\n",
      "RMSE: 4014752.39\n",
      "RMSE: 3110840.45\n",
      "RMSE: 1665022.76\n",
      "AVG RMSE: 2077492.67 MAX RMSE: 4014752.39 MIN RMSE: 1124963.23\n"
     ]
    }
   ],
   "source": [
    "# build model\n",
    "# all in one\n",
    "from sklearn import cross_validation, linear_model,ensemble\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeRegressor, ExtraTreeRegressor\n",
    "from sklearn.linear_model.stochastic_gradient import SGDRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "dataset = train_two.copy()\n",
    "Y = np.log(train_normal.revenue)\n",
    "\n",
    "kf = cross_validation.KFold(len(dataset), n_folds=10,shuffle=True)\n",
    "print kf\n",
    "\n",
    "# simple regression\n",
    "regr = linear_model.LinearRegression()\n",
    "avg_rmse = []\n",
    "\n",
    "print \"============\"\n",
    "print \"Simple regression\"\n",
    "print \"============\"\n",
    "\n",
    "for train_index, test_index in kf:\n",
    "    regr.fit(dataset.iloc[train_index], Y[train_index])\n",
    "    pred = np.exp(regr.predict(dataset.iloc[test_index]))\n",
    "    rmse = np.sqrt(np.mean((pred - np.exp(Y[test_index])) ** 2))\n",
    "    avg_rmse.append(rmse)\n",
    "    print(\"RMSE: %.2f\" % rmse)\n",
    "\n",
    "regr.fit(dataset,Y)\n",
    "\n",
    "print(\"AVG RMSE: %.2f MAX RMSE: %.2f MIN RMSE: %.2f\" % (np.mean(avg_rmse), np.max(avg_rmse), np.min(avg_rmse)))\n",
    "\n",
    "print \"============\"\n",
    "print \"Elastic net\"\n",
    "print \"============\"\n",
    "\n",
    "ratio = [.1, .5, .7, .9,.95, .99, 1] #.20,.35,.5,.75,\n",
    "elastic = linear_model.ElasticNetCV(l1_ratio=1, eps=0.0001, n_alphas=100, fit_intercept=True, normalize=True, precompute='auto', max_iter=1000, tol=0.0001, cv=8, copy_X=True)\n",
    "avg_rmse = []\n",
    "\n",
    "for train_index, test_index in kf:\n",
    "    elastic.fit(dataset.iloc[train_index], Y[train_index])\n",
    "    pred = np.exp(elastic.predict(dataset.iloc[test_index]))\n",
    "    rmse = np.sqrt(np.mean((pred - np.exp(Y[test_index])) ** 2))\n",
    "    avg_rmse.append(rmse)\n",
    "    print(\"RMSE: %.2f\" % rmse)\n",
    "    \n",
    "elastic.fit(dataset,Y)\n",
    "#dataset[\"elastic\"] = pd.Series(elastic.predict(dataset))\n",
    "print(\"AVG RMSE: %.2f MAX RMSE: %.2f MIN RMSE: %.2f\" % (np.mean(avg_rmse), np.max(avg_rmse), np.min(avg_rmse)))\n",
    "\n",
    "print \"============\"\n",
    "print \"GBM\"\n",
    "print \"============\"\n",
    "\n",
    "params = {'n_estimators': 2000, 'max_depth': 6, 'min_samples_split': 2,\n",
    "          'learning_rate': 0.001, 'loss': 'ls', 'min_samples_leaf':3}\n",
    "clf = ensemble.GradientBoostingRegressor(**params)\n",
    "avg_rmse = []\n",
    "\n",
    "for train_index, test_index in kf:\n",
    "    clf.fit(dataset.iloc[train_index], Y[train_index])\n",
    "    pred = np.exp(clf.predict(dataset.iloc[test_index]))\n",
    "    rmse = np.sqrt(np.mean((pred - np.exp(Y[test_index])) ** 2))\n",
    "    avg_rmse.append(rmse)\n",
    "    print(\"RMSE: %.2f\" % rmse)\n",
    "\n",
    "clf.fit(dataset,Y)\n",
    "print(\"AVG RMSE: %.2f MAX RMSE: %.2f MIN RMSE: %.2f\" % (np.mean(avg_rmse), np.max(avg_rmse), np.min(avg_rmse)))\n",
    "\n",
    "print \"============\"\n",
    "print \"AdaBoost\"\n",
    "print \"============\"\n",
    "\n",
    "\n",
    "ada_tr = ensemble.AdaBoostRegressor(DecisionTreeRegressor(max_depth=4),\n",
    "                          n_estimators=10, loss='square')\n",
    "avg_rmse = []\n",
    "\n",
    "for train_index, test_index in kf:\n",
    "    ada_tr.fit(dataset.iloc[train_index], Y[train_index])\n",
    "    pred = np.exp(ada_tr.predict(dataset.iloc[test_index]))\n",
    "    rmse = np.sqrt(np.mean((pred - np.exp(Y[test_index])) ** 2))\n",
    "    avg_rmse.append(rmse)\n",
    "    print(\"RMSE: %.2f\" % rmse)\n",
    "\n",
    "ada_tr.fit(dataset,Y)\n",
    "print(\"AVG RMSE: %.2f MAX RMSE: %.2f MIN RMSE: %.2f\" % (np.mean(avg_rmse), np.max(avg_rmse), np.min(avg_rmse)))\n",
    "\n",
    "print \"============\"\n",
    "print \"Extra Trees\"\n",
    "print \"============\"\n",
    "\n",
    "\n",
    "extr = ExtraTreeRegressor(criterion='mse', max_depth=6, min_samples_split=3, min_samples_leaf=3, max_features='auto')\n",
    "avg_rmse = []\n",
    "\n",
    "for train_index, test_index in kf:\n",
    "    extr.fit(dataset.iloc[train_index], Y[train_index])\n",
    "    pred = np.exp(extr.predict(dataset.iloc[test_index]))\n",
    "    rmse = np.sqrt(np.mean((pred - np.exp(Y[test_index])) ** 2))\n",
    "    avg_rmse.append(rmse)\n",
    "    print(\"RMSE: %.2f\" % rmse)\n",
    "\n",
    "extr.fit(dataset,Y)\n",
    "print(\"AVG RMSE: %.2f MAX RMSE: %.2f MIN RMSE: %.2f\" % (np.mean(avg_rmse), np.max(avg_rmse), np.min(avg_rmse)))\n",
    "\n",
    "\n",
    "print \"============\"\n",
    "print \"SGD\"\n",
    "print \"============\"\n",
    "\n",
    "sgd = SGDRegressor(penalty='elasticnet', alpha=0.01, l1_ratio=0.25, fit_intercept=True)\n",
    "avg_rmse = []\n",
    "\n",
    "for train_index, test_index in kf:\n",
    "    sgd.fit(dataset.iloc[train_index], Y[train_index])\n",
    "    pred = np.exp(sgd.predict(dataset.iloc[test_index]))\n",
    "    rmse = np.sqrt(np.mean((pred - np.exp(Y[test_index])) ** 2))\n",
    "    avg_rmse.append(rmse)\n",
    "    print(\"RMSE: %.2f\" % rmse)\n",
    "\n",
    "sgd.fit(dataset,Y)\n",
    "print(\"AVG RMSE: %.2f MAX RMSE: %.2f MIN RMSE: %.2f\" % (np.mean(avg_rmse), np.max(avg_rmse), np.min(avg_rmse)))\n",
    "\n",
    "print \"============\"\n",
    "print \"Random Forest\"\n",
    "print \"============\"\n",
    "rfr = ensemble.RandomForestRegressor(n_estimators=200, min_samples_leaf = 3, oob_score=True,  max_features=None)\n",
    "avg_rmse = []\n",
    "\n",
    "for train_index, test_index in kf:\n",
    "    rfr.fit(dataset.iloc[train_index], Y[train_index])\n",
    "    pred = np.exp(rfr.predict(dataset.iloc[test_index]))\n",
    "    rmse = np.sqrt(np.mean((pred - np.exp(Y[test_index])) ** 2))\n",
    "    avg_rmse.append(rmse)\n",
    "    print(\"RMSE: %.2f\" % rmse)\n",
    "\n",
    "rfr.fit(dataset,Y)\n",
    "print(\"AVG RMSE: %.2f MAX RMSE: %.2f MIN RMSE: %.2f\" % (np.mean(avg_rmse), np.max(avg_rmse), np.min(avg_rmse)))\n",
    "\n",
    "print \"============\"\n",
    "print \"SVR\"\n",
    "print \"============\"\n",
    "svr = SVR(C=1, epsilon=0.1)\n",
    "avg_rmse = []\n",
    "for train_index, test_index in kf:\n",
    "    svr.fit(dataset.iloc[train_index], Y[train_index])\n",
    "    pred = np.exp(svr.predict(dataset.iloc[test_index]))\n",
    "    rmse = np.sqrt(np.mean((pred - np.exp(Y[test_index])) ** 2))\n",
    "    avg_rmse.append(rmse)\n",
    "    print(\"RMSE: %.2f\" % rmse)\n",
    "    \n",
    "svr.fit(dataset,Y)\n",
    "print(\"AVG RMSE: %.2f MAX RMSE: %.2f MIN RMSE: %.2f\" % (np.mean(avg_rmse), np.max(avg_rmse), np.min(avg_rmse)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 15.4216888794\n",
      "2 1.47555748486\n",
      "4 4.97145929936\n",
      "5 1.53329790066\n",
      "14 2.92573333672\n",
      "15 5.87624666696\n",
      "37 1.13712895742\n",
      "46 3.08716245893\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-47e2366dcfb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/sklearn/ensemble/gradient_boosting.pyc\u001b[0m in \u001b[0;36mfeature_importances_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1146\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m             stage_sum = sum(tree.feature_importances_\n\u001b[0;32m-> 1148\u001b[0;31m                             for tree in stage) / len(stage)\n\u001b[0m\u001b[1;32m   1149\u001b[0m             \u001b[0mtotal_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mstage_sum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/sklearn/ensemble/gradient_boosting.pyc\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m((tree,))\u001b[0m\n\u001b[1;32m   1145\u001b[0m         \u001b[0mtotal_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m             stage_sum = sum(tree.feature_importances_\n\u001b[0m\u001b[1;32m   1148\u001b[0m                             for tree in stage) / len(stage)\n\u001b[1;32m   1149\u001b[0m             \u001b[0mtotal_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mstage_sum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(0, len(clf.feature_importances_)):\n",
    "    if clf.feature_importances_[i]*100 > 1:\n",
    "        print i, clf.feature_importances_[i]*100\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Number of features of the model must  match the input. Model n_features is 196 and  input n_features is 48 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-551307748859>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#dataset[\"regr\"] = pd.Series(regr.predict(dataset))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#prediction = dataset[\"regr\"] # regr.predict(dataset)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprediction1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrfr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Python/2.7/site-packages/sklearn/ensemble/forest.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    624\u001b[0m                              \u001b[0mbackend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"threading\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             \u001b[0mdelayed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_parallel_helper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'predict'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m             for e in self.estimators_)\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         \u001b[0;31m# Reduce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    657\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpre_dispatch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"all\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mdispatch\u001b[0;34m(self, func, args, kwargs)\u001b[0m\n\u001b[1;32m    404\u001b[0m         \"\"\"\n\u001b[1;32m    405\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_verbosity_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, args, kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/sklearn/ensemble/forest.pyc\u001b[0m in \u001b[0;36m_parallel_helper\u001b[0;34m(obj, methodname, *args, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_parallel_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethodname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;34m\"\"\"Private helper to workaround Python 2 pickle limitations\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethodname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/sklearn/tree/tree.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    344\u001b[0m                              \u001b[0;34m\" match the input. Model n_features is %s and \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m                              \u001b[0;34m\" input n_features is %s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m                              % (self.n_features_, n_features))\n\u001b[0m\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0mproba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Number of features of the model must  match the input. Model n_features is 196 and  input n_features is 48 "
     ]
    }
   ],
   "source": [
    "dataset = all_cat_test.copy()\n",
    "#dataset[\"regr\"] = pd.Series(regr.predict(dataset))\n",
    "#prediction = dataset[\"regr\"] # regr.predict(dataset)\n",
    "prediction1 = rfr.predict(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction = np.exp(prediction1)\n",
    "#prediction = prediction1\n",
    "prediction1 = np.exp(prediction1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: MacOSX\n",
      "7938037.41983\n",
      "1998296.63421\n",
      "100000\n",
      "0 0\n"
     ]
    }
   ],
   "source": [
    "%matplotlib\n",
    "print np.max(prediction1)\n",
    "print np.min(prediction1)\n",
    "#print pd.Series(prediction).hist(bins=100)\n",
    "print len(prediction1)\n",
    "ids = []\n",
    "ids2 = []\n",
    "for i in range(0,len(prediction)):\n",
    "    if prediction[i] > 20000000:\n",
    "        ids.append(i)\n",
    "    if prediction1[i] > 10000000:\n",
    "        ids2.append(i)\n",
    "print len(ids), len(ids2)\n",
    "slice_of_solution = test_normal.iloc[ids]\n",
    "#for name in slice_of_solution.keys():\n",
    "#    print name\n",
    "#    print dict(train_normal[name].value_counts())\n",
    "#    print dict(slice_of_solution[name].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 2)\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "name = \"./raw/my_submission_4_gbm_poly.csv\"\n",
    "result = open(name, \"w\")\n",
    "writer = csv.writer(result, delimiter=',')\n",
    "writer.writerow([\"Id\",\"Prediction\"])\n",
    "\n",
    "for i in range(0,len(prediction1)):\n",
    "    writer.writerow([i, prediction1[i]])\n",
    "result.close()    \n",
    "submis = pd.read_csv(name)\n",
    "print submis.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id   Open Date            City  City Group Type  P1  P2  P3  P4  P5 ...   \\\n",
      "0   0  01/22/2011           Niğde       Other   FC   1   4   4   4   1 ...    \n",
      "1   1  03/18/2011           Konya       Other   IL   3   4   4   4   2 ...    \n",
      "2   2  10/30/2013          Ankara  Big Cities   FC   3   4   4   4   2 ...    \n",
      "3   3  05/06/2013         Kocaeli       Other   IL   2   4   4   4   2 ...    \n",
      "4   4  07/31/2013  Afyonkarahisar       Other   FC   2   4   4   4   1 ...    \n",
      "\n",
      "   P28  P29  P30  P31  P32  P33  P34  P35  P36  P37  \n",
      "0    2    3    0    0    0    0    0    0    0    0  \n",
      "1    1    3    0    0    0    0    0    0    0    0  \n",
      "2    2    3    0    0    0    0    0    0    0    0  \n",
      "3    2    3    0    4    0    0    0    0    0    0  \n",
      "4    5    3    0    0    0    0    0    0    0    0  \n",
      "\n",
      "[5 rows x 42 columns]\n",
      "   Id   Open Date            City  City Group Type  P1  P2  P3  P4  P5 ...   \\\n",
      "0   0  01/22/2011           Niğde       Other   FC   1   4   4   4   1 ...    \n",
      "1   1  03/18/2011           Konya       Other   IL   3   4   4   4   2 ...    \n",
      "2   2  10/30/2013          Ankara  Big Cities   FC   3   4   4   4   2 ...    \n",
      "3   3  05/06/2013         Kocaeli       Other   IL   2   4   4   4   2 ...    \n",
      "4   4  07/31/2013  Afyonkarahisar       Other   FC   2   4   4   4   1 ...    \n",
      "\n",
      "   P28  P29  P30  P31  P32  P33  P34  P35  P36  P37  \n",
      "0    2    3    0    0    0    0    0    0    0    0  \n",
      "1    1    3    0    0    0    0    0    0    0    0  \n",
      "2    2    3    0    0    0    0    0    0    0    0  \n",
      "3    2    3    0    4    0    0    0    0    0    0  \n",
      "4    5    3    0    0    0    0    0    0    0    0  \n",
      "\n",
      "[5 rows x 42 columns]\n"
     ]
    }
   ],
   "source": [
    "submis1 = pd.read_csv(\"./raw/test_sneaky_not_null.csv\")\n",
    "submis2 = pd.read_csv(\"./raw/test.csv\")\n",
    "print submis1[0:5]\n",
    "print submis2[0:5]\n",
    "#k = submis1 - submis2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Id  Prediction\n",
      "0       0           0\n",
      "1       0           0\n",
      "2       0           0\n",
      "3       0           0\n",
      "4       0           0\n",
      "5       0           0\n",
      "6       0           0\n",
      "7       0           0\n",
      "8       0           0\n",
      "9       0           0\n",
      "10      0           0\n",
      "11      0           0\n",
      "12      0           0\n",
      "13      0           0\n",
      "14      0           0\n",
      "15      0           0\n",
      "16      0           0\n",
      "17      0           0\n",
      "18      0           0\n",
      "19      0           0\n",
      "20      0           0\n",
      "21      0           0\n",
      "22      0           0\n",
      "23      0           0\n",
      "24      0           0\n",
      "25      0           0\n",
      "26      0           0\n",
      "27      0           0\n",
      "28      0           0\n",
      "29      0           0\n",
      "...    ..         ...\n",
      "99970   0           0\n",
      "99971   0           0\n",
      "99972   0           0\n",
      "99973   0           0\n",
      "99974   0           0\n",
      "99975   0           0\n",
      "99976   0           0\n",
      "99977   0           0\n",
      "99978   0           0\n",
      "99979   0           0\n",
      "99980   0           0\n",
      "99981   0           0\n",
      "99982   0           0\n",
      "99983   0           0\n",
      "99984   0           0\n",
      "99985   0           0\n",
      "99986   0           0\n",
      "99987   0           0\n",
      "99988   0           0\n",
      "99989   0           0\n",
      "99990   0           0\n",
      "99991   0           0\n",
      "99992   0           0\n",
      "99993   0           0\n",
      "99994   0           0\n",
      "99995   0           0\n",
      "99996   0           0\n",
      "99997   0           0\n",
      "99998   0           0\n",
      "99999   0           0\n",
      "\n",
      "[100000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
